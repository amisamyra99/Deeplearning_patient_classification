{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "014da8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "672f4ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3529, 8)\n"
     ]
    }
   ],
   "source": [
    "Train = np.loadtxt('numerical_classification/train.csv')\n",
    "TrainLabel = np.loadtxt('numerical_classification/trainLabel.csv').astype(int)\n",
    "Test = np.loadtxt('numerical_classification/test.csv')\n",
    "TestLabel = np.loadtxt('numerical_classification/testLabel.csv').astype(int)\n",
    "print(Train.shape)\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(Train)\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(TrainLabel)\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(Test)\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(TestLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d40d74bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2823   706\n"
     ]
    }
   ],
   "source": [
    "p = int((len(Train)*0.8))\n",
    "\n",
    "Train_p = Train[:p]\n",
    "Val = Train[p:]\n",
    "\n",
    "TrainLabel_p = TrainLabel[:p]\n",
    "ValLabel = TrainLabel[p:]\n",
    "\n",
    "print(len(Train_p), \" \", len(Val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "312e1838",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatientDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.from_numpy(features).float()\n",
    "        self.labels = torch.from_numpy(labels).long()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        feature_vec = self.features[idx]\n",
    "        label = self.labels[idx]\n",
    "        return feature_vec, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "60dce561",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_dataloader = DataLoader(PatientDataset(Train_p, TrainLabel_p), batch_size=64)\n",
    "Val_dataloader = DataLoader(PatientDataset(Val, ValLabel), batch_size=64)\n",
    "Test_dataloader = DataLoader(PatientDataset(Test, TestLabel), batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8503df51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_dataloader.dataset.labels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f6fdefd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2823, 8])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_dataloader.dataset.features.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e22c6537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2823])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_dataloader.dataset.labels.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32d81d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryClassification, self).__init__()\n",
    "        # Number of input features is 8.\n",
    "      nn.Linear(12, 64) \n",
    "       nn.Linear(64, 64)\n",
    "        nn.Linear(64, 1) \n",
    "        \n",
    "        nn.ReLU()\n",
    "        nn.Dropout(p=0.1)\n",
    "       nn.BatchNorm1d(64)\n",
    "      = nn.BatchNorm1d(64)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "f6c28f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PNeuralNetwork,self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(8, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 1),\n",
    "            nn.Sigmoid(),\n",
    "           \n",
    "        \n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        prediction = self.model(x)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "2436b586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PNeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=8, out_features=8, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=8, out_features=1, bias=True)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "Model = PNeuralNetwork()\n",
    "print(Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "8808c231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pTraining(dataloader, model, loss_fn, optimizer):\n",
    "    full_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for (X, y) in dataloader:\n",
    "        \n",
    "        pred = model(X)\n",
    "        yf = y.type(torch.FloatTensor)\n",
    "        loss = loss_fn(pred, yf.unsqueeze(1)) # [32] -> [32, 1]\n",
    "        full_loss += loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        correct += (torch.flatten((pred > 0).long()) == y).type(torch.float).sum().item()\n",
    "    \n",
    "    full_loss /= len(dataloader)\n",
    "    correct /= len(dataloader.dataset)\n",
    "    print(\"Training Loss \", full_loss)\n",
    "    print(\"Training Accuracy \", correct)\n",
    "    return full_loss, correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "3c424491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pTesting(dataloader, model, loss_fn):\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    predLabels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            yf = y.type(torch.FloatTensor)\n",
    "            test_loss += loss_fn(pred, yf.unsqueeze(1)).item()\n",
    "            correct += (torch.flatten((pred > 0).long()) == y).type(torch.float).sum().item()\n",
    "            predLabels.append(torch.flatten((pred > 0).long()).tolist())\n",
    "    \n",
    "    test_loss /= len(dataloader)\n",
    "    correct /= len(dataloader.dataset)\n",
    "    print(\"Testing loss \", test_loss)\n",
    "    print(\"Testing Accuracy \", correct)\n",
    "    predLabels = list(np.concatenate(predLabels).flat)\n",
    "    return test_loss, correct, predLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "94b482d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 \n",
      "-----------------------------------\n",
      "Training Loss  0.6778733147664017\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.661435401957968\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  2 \n",
      "-----------------------------------\n",
      "Training Loss  0.6506485450133849\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.6320249749266583\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  3 \n",
      "-----------------------------------\n",
      "Training Loss  0.6236627584093073\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.6045269681059796\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  4 \n",
      "-----------------------------------\n",
      "Training Loss  0.6017865562037136\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.583597701528798\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  5 \n",
      "-----------------------------------\n",
      "Training Loss  0.586742621459318\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5697930947594021\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  6 \n",
      "-----------------------------------\n",
      "Training Loss  0.5770189340194959\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5611477442409681\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  7 \n",
      "-----------------------------------\n",
      "Training Loss  0.5708720968680435\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5558579123538473\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  8 \n",
      "-----------------------------------\n",
      "Training Loss  0.5667720370078355\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5524236300717229\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  9 \n",
      "-----------------------------------\n",
      "Training Loss  0.563963715949755\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5500215162401614\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  10 \n",
      "-----------------------------------\n",
      "Training Loss  0.5619713925243763\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5483718553315038\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  11 \n",
      "-----------------------------------\n",
      "Training Loss  0.5604131948412134\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5472124387388644\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  12 \n",
      "-----------------------------------\n",
      "Training Loss  0.5591049904233953\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5462278905122177\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  13 \n",
      "-----------------------------------\n",
      "Training Loss  0.5579298172104225\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5454115271568298\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  14 \n",
      "-----------------------------------\n",
      "Training Loss  0.55688090907054\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5447130527185358\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  15 \n",
      "-----------------------------------\n",
      "Training Loss  0.5558995524149263\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5441152233144512\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  16 \n",
      "-----------------------------------\n",
      "Training Loss  0.5549599143226495\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5435190148975538\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  17 \n",
      "-----------------------------------\n",
      "Training Loss  0.5540706562192252\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5431489115175994\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  18 \n",
      "-----------------------------------\n",
      "Training Loss  0.5532621198825622\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5427846144074979\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  19 \n",
      "-----------------------------------\n",
      "Training Loss  0.5525531296649676\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.542305728663569\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  20 \n",
      "-----------------------------------\n",
      "Training Loss  0.5518611810180578\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5418811090614485\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  21 \n",
      "-----------------------------------\n",
      "Training Loss  0.5512430178985167\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5416263315988623\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  22 \n",
      "-----------------------------------\n",
      "Training Loss  0.5506357932358645\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5413226770318073\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  23 \n",
      "-----------------------------------\n",
      "Training Loss  0.5500728970163324\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5409239362115446\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  24 \n",
      "-----------------------------------\n",
      "Training Loss  0.5495519912644719\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5405551830063695\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  25 \n",
      "-----------------------------------\n",
      "Training Loss  0.5490816664159968\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5402247879816138\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  26 \n",
      "-----------------------------------\n",
      "Training Loss  0.5486352895752767\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5399102879607159\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  27 \n",
      "-----------------------------------\n",
      "Training Loss  0.5482117865192756\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5396318733692169\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  28 \n",
      "-----------------------------------\n",
      "Training Loss  0.5478272354334928\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5394253471623296\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  29 \n",
      "-----------------------------------\n",
      "Training Loss  0.5474213395225868\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5391929732716602\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  30 \n",
      "-----------------------------------\n",
      "Training Loss  0.5470195724053329\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5390111881753673\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  31 \n",
      "-----------------------------------\n",
      "Training Loss  0.5466355838802424\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5387812204982924\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  32 \n",
      "-----------------------------------\n",
      "Training Loss  0.5462830558921514\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5385421164657759\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  33 \n",
      "-----------------------------------\n",
      "Training Loss  0.5459846800632691\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5383049314436705\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  34 \n",
      "-----------------------------------\n",
      "Training Loss  0.5456947791442442\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5379834706368654\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  35 \n",
      "-----------------------------------\n",
      "Training Loss  0.5454024783011233\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.537665755852409\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  36 \n",
      "-----------------------------------\n",
      "Training Loss  0.5451018626100561\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5373544498630192\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  37 \n",
      "-----------------------------------\n",
      "Training Loss  0.5447990469048533\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5370302485383075\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  38 \n",
      "-----------------------------------\n",
      "Training Loss  0.5444866582918703\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5366381523401841\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  39 \n",
      "-----------------------------------\n",
      "Training Loss  0.544188138809097\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5364287288292594\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  40 \n",
      "-----------------------------------\n",
      "Training Loss  0.5439308255576016\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5362077096234197\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  41 \n",
      "-----------------------------------\n",
      "Training Loss  0.5436640137367035\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5360082115816034\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  42 \n",
      "-----------------------------------\n",
      "Training Loss  0.5433742906270402\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5358447575050852\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  43 \n",
      "-----------------------------------\n",
      "Training Loss  0.5430724701184905\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5357140211955361\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  44 \n",
      "-----------------------------------\n",
      "Training Loss  0.5428082454070616\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5355172507140947\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  45 \n",
      "-----------------------------------\n",
      "Training Loss  0.5425275738319654\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5353999785754991\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  46 \n",
      "-----------------------------------\n",
      "Training Loss  0.5422742775316989\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.535278355297835\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  47 \n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss  0.5420218518610751\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5351406271043031\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  48 \n",
      "-----------------------------------\n",
      "Training Loss  0.5417715230684602\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5351777050806128\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  49 \n",
      "-----------------------------------\n",
      "Training Loss  0.5415366787588998\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.535138735304708\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  50 \n",
      "-----------------------------------\n",
      "Training Loss  0.5412922823027279\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5350898504257202\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  51 \n",
      "-----------------------------------\n",
      "Training Loss  0.5410782256153193\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5349904855956202\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  52 \n",
      "-----------------------------------\n",
      "Training Loss  0.5408873973267802\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5349088997944541\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  53 \n",
      "-----------------------------------\n",
      "Training Loss  0.5407004088498233\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5348954058211782\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  54 \n",
      "-----------------------------------\n",
      "Training Loss  0.5405089804965458\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5348026882047239\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  55 \n",
      "-----------------------------------\n",
      "Training Loss  0.5403447589847479\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5347342568895092\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  56 \n",
      "-----------------------------------\n",
      "Training Loss  0.5401674277996749\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5346039442912393\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  57 \n",
      "-----------------------------------\n",
      "Training Loss  0.5399947236762958\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5345601335815762\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  58 \n",
      "-----------------------------------\n",
      "Training Loss  0.5398209744624878\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5344507940437483\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  59 \n",
      "-----------------------------------\n",
      "Training Loss  0.5396565054909567\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5343666504258695\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  60 \n",
      "-----------------------------------\n",
      "Training Loss  0.5395202244935411\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5342954565649447\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  61 \n",
      "-----------------------------------\n",
      "Training Loss  0.539357362503416\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5342286630817081\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  62 \n",
      "-----------------------------------\n",
      "Training Loss  0.5392109560832549\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5341787532619808\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  63 \n",
      "-----------------------------------\n",
      "Training Loss  0.5390442726987131\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5341160077115764\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  64 \n",
      "-----------------------------------\n",
      "Training Loss  0.5388614130154085\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5340160364690034\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  65 \n",
      "-----------------------------------\n",
      "Training Loss  0.5386741221620795\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.533941766490107\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  66 \n",
      "-----------------------------------\n",
      "Training Loss  0.5385152620545933\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5339296226916106\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  67 \n",
      "-----------------------------------\n",
      "Training Loss  0.5383269247714053\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5338559047035549\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  68 \n",
      "-----------------------------------\n",
      "Training Loss  0.5381855006968037\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5338334557802781\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  69 \n",
      "-----------------------------------\n",
      "Training Loss  0.5380229330464695\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5337959398394045\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  70 \n",
      "-----------------------------------\n",
      "Training Loss  0.5378825949149185\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5338127664897753\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  71 \n",
      "-----------------------------------\n",
      "Training Loss  0.5377410998505153\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5338164516117262\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  72 \n",
      "-----------------------------------\n",
      "Training Loss  0.5376127921463398\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5337956262671429\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  73 \n",
      "-----------------------------------\n",
      "Training Loss  0.5374841043788395\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5337289908657903\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  74 \n",
      "-----------------------------------\n",
      "Training Loss  0.5373506338408823\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5337165477483169\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  75 \n",
      "-----------------------------------\n",
      "Training Loss  0.5372002606981257\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5336886722108592\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  76 \n",
      "-----------------------------------\n",
      "Training Loss  0.5370209039597029\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.533675078464591\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  77 \n",
      "-----------------------------------\n",
      "Training Loss  0.5368718149956693\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5336600321790447\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  78 \n",
      "-----------------------------------\n",
      "Training Loss  0.5367066649238715\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5336726722509965\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  79 \n",
      "-----------------------------------\n",
      "Training Loss  0.5365487430872542\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5336958869643833\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  80 \n",
      "-----------------------------------\n",
      "Training Loss  0.5363799845904447\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5337128768796506\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  81 \n",
      "-----------------------------------\n",
      "Training Loss  0.5362293696805333\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5336690013823302\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  82 \n",
      "-----------------------------------\n",
      "Training Loss  0.5360974525467733\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5336477095666139\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  83 \n",
      "-----------------------------------\n",
      "Training Loss  0.5359884594263655\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.533577359240988\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  84 \n",
      "-----------------------------------\n",
      "Training Loss  0.5358739445048771\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5336254925831504\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  85 \n",
      "-----------------------------------\n",
      "Training Loss  0.5357303060172649\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5336980197740637\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  86 \n",
      "-----------------------------------\n",
      "Training Loss  0.5356180952506119\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5337572654952174\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  87 \n",
      "-----------------------------------\n",
      "Training Loss  0.5354997344901052\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5337999968425088\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  88 \n",
      "-----------------------------------\n",
      "Training Loss  0.5354006200024252\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5338501411935558\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  89 \n",
      "-----------------------------------\n",
      "Training Loss  0.5353033636393172\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5339030247667561\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  90 \n",
      "-----------------------------------\n",
      "Training Loss  0.5352068670680014\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5339215384877246\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  91 \n",
      "-----------------------------------\n",
      "Training Loss  0.5351139672016829\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5339767401633055\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  92 \n",
      "-----------------------------------\n",
      "Training Loss  0.5350373665268502\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.53404417504435\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  93 \n",
      "-----------------------------------\n",
      "Training Loss  0.5349405260568254\n",
      "Training Accuracy  0.5990081473609635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss  0.5341038263362387\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  94 \n",
      "-----------------------------------\n",
      "Training Loss  0.5348326182097531\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5341275269570558\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  95 \n",
      "-----------------------------------\n",
      "Training Loss  0.5347440410196111\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5342998789704364\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  96 \n",
      "-----------------------------------\n",
      "Training Loss  0.5346447021773691\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.53436697177265\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  97 \n",
      "-----------------------------------\n",
      "Training Loss  0.5345430631985825\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5344270053117172\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  98 \n",
      "-----------------------------------\n",
      "Training Loss  0.5344582816188255\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5344864285510519\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  99 \n",
      "-----------------------------------\n",
      "Training Loss  0.5343572095538793\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5345631332501121\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  100 \n",
      "-----------------------------------\n",
      "Training Loss  0.5343032174565819\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5346840555253236\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  101 \n",
      "-----------------------------------\n",
      "Training Loss  0.5342220700858684\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5347017995689226\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  102 \n",
      "-----------------------------------\n",
      "Training Loss  0.5341360187262632\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5347311846587969\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  103 \n",
      "-----------------------------------\n",
      "Training Loss  0.5340433800488376\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.534762107807657\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  104 \n",
      "-----------------------------------\n",
      "Training Loss  0.5339677330483211\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5347581583520641\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  105 \n",
      "-----------------------------------\n",
      "Training Loss  0.5339035572630636\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5347576724446338\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  106 \n",
      "-----------------------------------\n",
      "Training Loss  0.533831283952413\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5348182670448137\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  107 \n",
      "-----------------------------------\n",
      "Training Loss  0.533756042129538\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5347924711911575\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  108 \n",
      "-----------------------------------\n",
      "Training Loss  0.5337076692768697\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5348028268503107\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  109 \n",
      "-----------------------------------\n",
      "Training Loss  0.533641614940729\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5348327250584312\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  110 \n",
      "-----------------------------------\n",
      "Training Loss  0.5335762648100264\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5348708992419036\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  111 \n",
      "-----------------------------------\n",
      "Training Loss  0.5335525800003095\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5349282013333362\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  112 \n",
      "-----------------------------------\n",
      "Training Loss  0.5334718471832489\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5348587152750596\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  113 \n",
      "-----------------------------------\n",
      "Training Loss  0.5334482032261537\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5349529787250187\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  114 \n",
      "-----------------------------------\n",
      "Training Loss  0.5333688292610511\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5349132314972256\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  115 \n",
      "-----------------------------------\n",
      "Training Loss  0.5333389445637049\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5349867706713469\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  116 \n",
      "-----------------------------------\n",
      "Training Loss  0.5332848124959496\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5350102834079576\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  117 \n",
      "-----------------------------------\n",
      "Training Loss  0.5332391771037926\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5350811170495074\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  118 \n",
      "-----------------------------------\n",
      "Training Loss  0.5331968681865864\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5351060913956683\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  119 \n",
      "-----------------------------------\n",
      "Training Loss  0.5331421460998192\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5351615625879039\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  120 \n",
      "-----------------------------------\n",
      "Training Loss  0.5331011816356959\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5351857892844988\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  121 \n",
      "-----------------------------------\n",
      "Training Loss  0.5330575064996655\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.535258359235266\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  122 \n",
      "-----------------------------------\n",
      "Training Loss  0.5330140567227696\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5353099224360093\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  123 \n",
      "-----------------------------------\n",
      "Training Loss  0.5329869329259637\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.535401314496994\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  124 \n",
      "-----------------------------------\n",
      "Training Loss  0.532930598165212\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5354308887668278\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  125 \n",
      "-----------------------------------\n",
      "Training Loss  0.5329245429360465\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5355035014774489\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  126 \n",
      "-----------------------------------\n",
      "Training Loss  0.5328727163625567\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5354834794998169\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  127 \n",
      "-----------------------------------\n",
      "Training Loss  0.5328468944919243\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5355093064515487\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  128 \n",
      "-----------------------------------\n",
      "Training Loss  0.532821457037765\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.535495198291281\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  129 \n",
      "-----------------------------------\n",
      "Training Loss  0.5327946804882435\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5355389325515084\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  130 \n",
      "-----------------------------------\n",
      "Training Loss  0.5327565144287066\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5355993548165197\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  131 \n",
      "-----------------------------------\n",
      "Training Loss  0.532735532589173\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5356242721495421\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  132 \n",
      "-----------------------------------\n",
      "Training Loss  0.532703054420064\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.535670157359994\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  133 \n",
      "-----------------------------------\n",
      "Training Loss  0.532675756832187\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5356940899206244\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  134 \n",
      "-----------------------------------\n",
      "Training Loss  0.5326503321026148\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.535739796317142\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  135 \n",
      "-----------------------------------\n",
      "Training Loss  0.5326168296042453\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5357745082482047\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  136 \n",
      "-----------------------------------\n",
      "Training Loss  0.5325928126158339\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5358109966568325\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  137 \n",
      "-----------------------------------\n",
      "Training Loss  0.5325656328978163\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5358402819737144\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  138 \n",
      "-----------------------------------\n",
      "Training Loss  0.5325443423196171\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5358766783838687\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  139 \n",
      "-----------------------------------\n",
      "Training Loss  0.5324950643469778\n",
      "Training Accuracy  0.5990081473609635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss  0.5359073579311371\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  140 \n",
      "-----------------------------------\n",
      "Training Loss  0.5324600228432859\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5359413546064625\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  141 \n",
      "-----------------------------------\n",
      "Training Loss  0.532418225253566\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5359789016454116\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  142 \n",
      "-----------------------------------\n",
      "Training Loss  0.5323731989673014\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.536036482323771\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  143 \n",
      "-----------------------------------\n",
      "Training Loss  0.5323365401016192\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5361264762671097\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  144 \n",
      "-----------------------------------\n",
      "Training Loss  0.5322921687967321\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5361623647420303\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  145 \n",
      "-----------------------------------\n",
      "Training Loss  0.5322493626160568\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5362082732760388\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  146 \n",
      "-----------------------------------\n",
      "Training Loss  0.5321930208232966\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5362628931584565\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  147 \n",
      "-----------------------------------\n",
      "Training Loss  0.5321486970681822\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5363230873709139\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  148 \n",
      "-----------------------------------\n",
      "Training Loss  0.5320984627423662\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5363716146220332\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  149 \n",
      "-----------------------------------\n",
      "Training Loss  0.5320465296841739\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5364092264486395\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  150 \n",
      "-----------------------------------\n",
      "Training Loss  0.5319759574499023\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5364633876344432\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  151 \n",
      "-----------------------------------\n",
      "Training Loss  0.5319174829493748\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5365079097125841\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  152 \n",
      "-----------------------------------\n",
      "Training Loss  0.531859495331732\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5365596569102743\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  153 \n",
      "-----------------------------------\n",
      "Training Loss  0.5317964892039139\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5366237357906674\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  154 \n",
      "-----------------------------------\n",
      "Training Loss  0.5317239486769344\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5366650793863379\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  155 \n",
      "-----------------------------------\n",
      "Training Loss  0.5316407760877288\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5367312833018925\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  156 \n",
      "-----------------------------------\n",
      "Training Loss  0.5315545639964971\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5367837042912192\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  157 \n",
      "-----------------------------------\n",
      "Training Loss  0.5314624761597494\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.536855408678884\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  158 \n",
      "-----------------------------------\n",
      "Training Loss  0.5313581047433146\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.536921774563582\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  159 \n",
      "-----------------------------------\n",
      "Training Loss  0.5312626646475845\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5369709691275721\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  160 \n",
      "-----------------------------------\n",
      "Training Loss  0.5311749225921845\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5370350972465847\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  161 \n",
      "-----------------------------------\n",
      "Training Loss  0.5310846627428291\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5371159716792728\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  162 \n",
      "-----------------------------------\n",
      "Training Loss  0.5309893398472433\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5371746887331423\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  163 \n",
      "-----------------------------------\n",
      "Training Loss  0.5308932996867748\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5372430228668711\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  164 \n",
      "-----------------------------------\n",
      "Training Loss  0.5308128592003597\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5373253718666409\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  165 \n",
      "-----------------------------------\n",
      "Training Loss  0.5307385623455048\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.53736393995907\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  166 \n",
      "-----------------------------------\n",
      "Training Loss  0.5306609915883354\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5374791233435922\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  167 \n",
      "-----------------------------------\n",
      "Training Loss  0.5306053643815973\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5375678850256879\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  168 \n",
      "-----------------------------------\n",
      "Training Loss  0.5305147874221373\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.537657107995904\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  169 \n",
      "-----------------------------------\n",
      "Training Loss  0.5304610324039888\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5377541741599208\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  170 \n",
      "-----------------------------------\n",
      "Training Loss  0.5304028495643915\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5378896334896917\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  171 \n",
      "-----------------------------------\n",
      "Training Loss  0.5303289230619923\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5379713527534319\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  172 \n",
      "-----------------------------------\n",
      "Training Loss  0.5302698106578226\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5381006637345189\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  173 \n",
      "-----------------------------------\n",
      "Training Loss  0.5302010865023966\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5381759923437367\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  174 \n",
      "-----------------------------------\n",
      "Training Loss  0.5301562661535284\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5382822583550992\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  175 \n",
      "-----------------------------------\n",
      "Training Loss  0.5300823118579522\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5383660262045653\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  176 \n",
      "-----------------------------------\n",
      "Training Loss  0.5300505047433832\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5384164633958236\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  177 \n",
      "-----------------------------------\n",
      "Training Loss  0.5299923751461372\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.538528578436893\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  178 \n",
      "-----------------------------------\n",
      "Training Loss  0.5299371032232649\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5385613273019376\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  179 \n",
      "-----------------------------------\n",
      "Training Loss  0.529901723178585\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5387010159699813\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  180 \n",
      "-----------------------------------\n",
      "Training Loss  0.529851560177428\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.538783205592114\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  181 \n",
      "-----------------------------------\n",
      "Training Loss  0.5297884194368727\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5388052152550739\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  182 \n",
      "-----------------------------------\n",
      "Training Loss  0.5297432160109616\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5389110471891321\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  183 \n",
      "-----------------------------------\n",
      "Training Loss  0.5297006720237518\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.539003479739894\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  184 \n",
      "-----------------------------------\n",
      "Training Loss  0.5296313685647557\n",
      "Training Accuracy  0.5990081473609635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss  0.5390386710996213\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  185 \n",
      "-----------------------------------\n",
      "Training Loss  0.529602761014124\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5390666541845902\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  186 \n",
      "-----------------------------------\n",
      "Training Loss  0.5295583203937231\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.539156683113264\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  187 \n",
      "-----------------------------------\n",
      "Training Loss  0.5295214435357726\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5391691987929137\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  188 \n",
      "-----------------------------------\n",
      "Training Loss  0.5294902405042327\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5392683705557948\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  189 \n",
      "-----------------------------------\n",
      "Training Loss  0.5294474828779028\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.53929338377455\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  190 \n",
      "-----------------------------------\n",
      "Training Loss  0.5294030265191968\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5393558753573376\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  191 \n",
      "-----------------------------------\n",
      "Training Loss  0.5293909093637145\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5394424264845641\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  192 \n",
      "-----------------------------------\n",
      "Training Loss  0.5293506142798434\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5395197635111602\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  193 \n",
      "-----------------------------------\n",
      "Training Loss  0.5293156035830465\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5395328881947891\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  194 \n",
      "-----------------------------------\n",
      "Training Loss  0.5292825019091703\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.539608341196309\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  195 \n",
      "-----------------------------------\n",
      "Training Loss  0.5292579116446249\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5396217068900233\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  196 \n",
      "-----------------------------------\n",
      "Training Loss  0.5292271606707841\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5397204575331315\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  197 \n",
      "-----------------------------------\n",
      "Training Loss  0.5291837098893155\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5397320869176284\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  198 \n",
      "-----------------------------------\n",
      "Training Loss  0.529169752691569\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5398139655590057\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  199 \n",
      "-----------------------------------\n",
      "Training Loss  0.5291352138090669\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5398206555325052\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  200 \n",
      "-----------------------------------\n",
      "Training Loss  0.5291027785017249\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5398927784484365\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  201 \n",
      "-----------------------------------\n",
      "Training Loss  0.5290812039643191\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5399151962736378\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  202 \n",
      "-----------------------------------\n",
      "Training Loss  0.5290588277779268\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5400051202463068\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  203 \n",
      "-----------------------------------\n",
      "Training Loss  0.529019070475289\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5400042805982672\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  204 \n",
      "-----------------------------------\n",
      "Training Loss  0.5289977705210782\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5400990468004475\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  205 \n",
      "-----------------------------------\n",
      "Training Loss  0.528974922520391\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.540155458709468\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  206 \n",
      "-----------------------------------\n",
      "Training Loss  0.5289436900213863\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5401802568332009\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  207 \n",
      "-----------------------------------\n",
      "Training Loss  0.5289139024327311\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5402346590290898\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  208 \n",
      "-----------------------------------\n",
      "Training Loss  0.5289003306560303\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5403081567391105\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  209 \n",
      "-----------------------------------\n",
      "Training Loss  0.5288795259561432\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5403426328431005\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  210 \n",
      "-----------------------------------\n",
      "Training Loss  0.5288612246513367\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5403883081415425\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  211 \n",
      "-----------------------------------\n",
      "Training Loss  0.528843074702145\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5404363598512567\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  212 \n",
      "-----------------------------------\n",
      "Training Loss  0.5288116506646189\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.540475483821786\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  213 \n",
      "-----------------------------------\n",
      "Training Loss  0.5288043303436107\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5405343136061793\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  214 \n",
      "-----------------------------------\n",
      "Training Loss  0.5287865648108921\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5405640874219977\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  215 \n",
      "-----------------------------------\n",
      "Training Loss  0.5287376956993275\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5405607379001119\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  216 \n",
      "-----------------------------------\n",
      "Training Loss  0.528741034898865\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.540646405323692\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  217 \n",
      "-----------------------------------\n",
      "Training Loss  0.5286963692541873\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5407132091729537\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  218 \n",
      "-----------------------------------\n",
      "Training Loss  0.5286836135253478\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.540770077187082\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  219 \n",
      "-----------------------------------\n",
      "Training Loss  0.5286643779679631\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5408611932526464\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  220 \n",
      "-----------------------------------\n",
      "Training Loss  0.5286373328626826\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5409043418324512\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  221 \n",
      "-----------------------------------\n",
      "Training Loss  0.5286239234918959\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5409419873486394\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  222 \n",
      "-----------------------------------\n",
      "Training Loss  0.5286033474997188\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5410332627918409\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  223 \n",
      "-----------------------------------\n",
      "Training Loss  0.5285884560493941\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5411116338294485\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  224 \n",
      "-----------------------------------\n",
      "Training Loss  0.5285724210605193\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.541145665490109\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  225 \n",
      "-----------------------------------\n",
      "Training Loss  0.5285597001568655\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5412262224632761\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  226 \n",
      "-----------------------------------\n",
      "Training Loss  0.5285404927275154\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5412894448508387\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  227 \n",
      "-----------------------------------\n",
      "Training Loss  0.5285231802570686\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5413161891957988\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  228 \n",
      "-----------------------------------\n",
      "Training Loss  0.5285214875521285\n",
      "Training Accuracy  0.5990081473609635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss  0.5413979382618613\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  229 \n",
      "-----------------------------------\n",
      "Training Loss  0.5284939863708582\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5414619601291158\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  230 \n",
      "-----------------------------------\n",
      "Training Loss  0.5284724426403474\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5414925023265507\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  231 \n",
      "-----------------------------------\n",
      "Training Loss  0.5284683687633343\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.541557692963144\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  232 \n",
      "-----------------------------------\n",
      "Training Loss  0.5284488013621127\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5416071427905041\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  233 \n",
      "-----------------------------------\n",
      "Training Loss  0.5284285294205955\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5416145946668542\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  234 \n",
      "-----------------------------------\n",
      "Training Loss  0.5284190261631869\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5416567869808363\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  235 \n",
      "-----------------------------------\n",
      "Training Loss  0.5284088826581333\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5417412765648054\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  236 \n",
      "-----------------------------------\n",
      "Training Loss  0.5283876399645645\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5417334022729293\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  237 \n",
      "-----------------------------------\n",
      "Training Loss  0.5283863731984342\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5418011196281599\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  238 \n",
      "-----------------------------------\n",
      "Training Loss  0.5283612528543794\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5418471849482992\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  239 \n",
      "-----------------------------------\n",
      "Training Loss  0.5283633835530013\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5418982726076375\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  240 \n",
      "-----------------------------------\n",
      "Training Loss  0.5283432736825407\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5419437133747599\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  241 \n",
      "-----------------------------------\n",
      "Training Loss  0.5283291005016713\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5419966591441113\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  242 \n",
      "-----------------------------------\n",
      "Training Loss  0.5283217051725709\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5420084699340488\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  243 \n",
      "-----------------------------------\n",
      "Training Loss  0.5283113878764464\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5421085279920826\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  244 \n",
      "-----------------------------------\n",
      "Training Loss  0.5282967974630635\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5421450345412545\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  245 \n",
      "-----------------------------------\n",
      "Training Loss  0.5282940951625953\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5422196919503419\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  246 \n",
      "-----------------------------------\n",
      "Training Loss  0.5282845423462685\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.542245925768562\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  247 \n",
      "-----------------------------------\n",
      "Training Loss  0.5282654681902254\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.542256227006083\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  248 \n",
      "-----------------------------------\n",
      "Training Loss  0.5282561199718647\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5423000118006831\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  249 \n",
      "-----------------------------------\n",
      "Training Loss  0.5282481272568863\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5423237862794296\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  250 \n",
      "-----------------------------------\n",
      "Training Loss  0.5282543566119805\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.54236914800561\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  251 \n",
      "-----------------------------------\n",
      "Training Loss  0.5282238249698382\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5423890598442244\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  252 \n",
      "-----------------------------------\n",
      "Training Loss  0.528227439422286\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5424007410588472\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  253 \n",
      "-----------------------------------\n",
      "Training Loss  0.5282009441531106\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5424057672853055\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  254 \n",
      "-----------------------------------\n",
      "Training Loss  0.5282124190518026\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5424584785233373\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  255 \n",
      "-----------------------------------\n",
      "Training Loss  0.5281840599654766\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5424554684887761\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  256 \n",
      "-----------------------------------\n",
      "Training Loss  0.5281901332769501\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5424771308898926\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  257 \n",
      "-----------------------------------\n",
      "Training Loss  0.5281773611400904\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5424866235774496\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  258 \n",
      "-----------------------------------\n",
      "Training Loss  0.5281576962953203\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5424753272015116\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  259 \n",
      "-----------------------------------\n",
      "Training Loss  0.5281495220875472\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5424775170243304\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  260 \n",
      "-----------------------------------\n",
      "Training Loss  0.5281445782506065\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5424831276354583\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  261 \n",
      "-----------------------------------\n",
      "Training Loss  0.5281385789426525\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5425735245580259\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  262 \n",
      "-----------------------------------\n",
      "Training Loss  0.528130362877685\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5425575984560925\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  263 \n",
      "-----------------------------------\n",
      "Training Loss  0.5281175405121921\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5426014260105465\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  264 \n",
      "-----------------------------------\n",
      "Training Loss  0.5281296184893405\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5426343964493793\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  265 \n",
      "-----------------------------------\n",
      "Training Loss  0.5281017941705296\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5426085085972495\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  266 \n",
      "-----------------------------------\n",
      "Training Loss  0.5280894858113835\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.542623405871184\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  267 \n",
      "-----------------------------------\n",
      "Training Loss  0.5280991260255321\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5426195600758428\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  268 \n",
      "-----------------------------------\n",
      "Training Loss  0.5280926800175999\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5426611589348834\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  269 \n",
      "-----------------------------------\n",
      "Training Loss  0.5280773385187213\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.542669696652371\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  270 \n",
      "-----------------------------------\n",
      "Training Loss  0.5280640845218402\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5426790040472279\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  271 \n",
      "-----------------------------------\n",
      "Training Loss  0.5280686610200432\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5426851290723552\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  272 \n",
      "-----------------------------------\n",
      "Training Loss  0.5280548786179403\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5427330589812734\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  273 \n",
      "-----------------------------------\n",
      "Training Loss  0.5280615134855334\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5427554975385251\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  274 \n",
      "-----------------------------------\n",
      "Training Loss  0.5280421818240305\n",
      "Training Accuracy  0.5990081473609635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss  0.5427623818749967\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  275 \n",
      "-----------------------------------\n",
      "Training Loss  0.5280259094211492\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5427727349426436\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  276 \n",
      "-----------------------------------\n",
      "Training Loss  0.5280334721790271\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5427819347899893\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  277 \n",
      "-----------------------------------\n",
      "Training Loss  0.5280309771553854\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5428190399771151\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  278 \n",
      "-----------------------------------\n",
      "Training Loss  0.5280135079716028\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5427541642085366\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  279 \n",
      "-----------------------------------\n",
      "Training Loss  0.5280079865053798\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5427977986957716\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  280 \n",
      "-----------------------------------\n",
      "Training Loss  0.5280064863435338\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5428037565687428\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  281 \n",
      "-----------------------------------\n",
      "Training Loss  0.5280128727468212\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5428336327490599\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  282 \n",
      "-----------------------------------\n",
      "Training Loss  0.5279908739448933\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5428316087826438\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  283 \n",
      "-----------------------------------\n",
      "Training Loss  0.5279761388730467\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5428253632524739\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  284 \n",
      "-----------------------------------\n",
      "Training Loss  0.5279791378573085\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5428356554197229\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  285 \n",
      "-----------------------------------\n",
      "Training Loss  0.5279677312695579\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5428756605023923\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  286 \n",
      "-----------------------------------\n",
      "Training Loss  0.5279804190223136\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5429214964742246\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  287 \n",
      "-----------------------------------\n",
      "Training Loss  0.5279583020156688\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5429103944612585\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  288 \n",
      "-----------------------------------\n",
      "Training Loss  0.5279613944251885\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5429129781930343\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  289 \n",
      "-----------------------------------\n",
      "Training Loss  0.527939281771692\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5429321397905764\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  290 \n",
      "-----------------------------------\n",
      "Training Loss  0.5279577176892356\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5429457153963007\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  291 \n",
      "-----------------------------------\n",
      "Training Loss  0.5279340908098756\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5429492592811584\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  292 \n",
      "-----------------------------------\n",
      "Training Loss  0.5279301796736342\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5429997988369154\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  293 \n",
      "-----------------------------------\n",
      "Training Loss  0.5279297095336272\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5429585290991742\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  294 \n",
      "-----------------------------------\n",
      "Training Loss  0.527925986252474\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5429990861726843\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  295 \n",
      "-----------------------------------\n",
      "Training Loss  0.5279184800855229\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5430213161136793\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  296 \n",
      "-----------------------------------\n",
      "Training Loss  0.5279042807857642\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5430002575335295\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  297 \n",
      "-----------------------------------\n",
      "Training Loss  0.5279106531250343\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5430134392302969\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  298 \n",
      "-----------------------------------\n",
      "Training Loss  0.5279021587934387\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5429974172426306\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  299 \n",
      "-----------------------------------\n",
      "Training Loss  0.5278842814182967\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5429622880790544\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  300 \n",
      "-----------------------------------\n",
      "Training Loss  0.5278985175523865\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5429622971493265\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  301 \n",
      "-----------------------------------\n",
      "Training Loss  0.5278758312878984\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5429654004781143\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  302 \n",
      "-----------------------------------\n",
      "Training Loss  0.5278656449210778\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5429217867229296\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  303 \n",
      "-----------------------------------\n",
      "Training Loss  0.5278854614563202\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5429554827835249\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  304 \n",
      "-----------------------------------\n",
      "Training Loss  0.5278510005956285\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.542966902256012\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  305 \n",
      "-----------------------------------\n",
      "Training Loss  0.5278620281246271\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5429619900558306\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  306 \n",
      "-----------------------------------\n",
      "Training Loss  0.5278604067443462\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5429853654426077\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  307 \n",
      "-----------------------------------\n",
      "Training Loss  0.5278399288654327\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5429773848989735\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  308 \n",
      "-----------------------------------\n",
      "Training Loss  0.5278632965650452\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5430373730866805\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  309 \n",
      "-----------------------------------\n",
      "Training Loss  0.5278308153822181\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5430588903634445\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  310 \n",
      "-----------------------------------\n",
      "Training Loss  0.5278376454717657\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5429660276226376\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  311 \n",
      "-----------------------------------\n",
      "Training Loss  0.5278362260105904\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5430160501728887\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  312 \n",
      "-----------------------------------\n",
      "Training Loss  0.5278266219610579\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543030260697655\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  313 \n",
      "-----------------------------------\n",
      "Training Loss  0.5278308569045549\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543023055014403\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  314 \n",
      "-----------------------------------\n",
      "Training Loss  0.5278159718165237\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5430273102677386\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  315 \n",
      "-----------------------------------\n",
      "Training Loss  0.5278217146235905\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5430785119533539\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  316 \n",
      "-----------------------------------\n",
      "Training Loss  0.5278122371502136\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5430582126845485\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  317 \n",
      "-----------------------------------\n",
      "Training Loss  0.5278029525547885\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5430217061353766\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  318 \n",
      "-----------------------------------\n",
      "Training Loss  0.5278076546245747\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5430519114369932\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  319 \n",
      "-----------------------------------\n",
      "Training Loss  0.5277953988380646\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5430807043676791\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  320 \n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss  0.5278008519933465\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5430523869783982\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  321 \n",
      "-----------------------------------\n",
      "Training Loss  0.5277955173776391\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5431008066820062\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  322 \n",
      "-----------------------------------\n",
      "Training Loss  0.5277821203965819\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5430811578812806\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  323 \n",
      "-----------------------------------\n",
      "Training Loss  0.5277807943606645\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5431158153907113\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  324 \n",
      "-----------------------------------\n",
      "Training Loss  0.5277621190199692\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5431068345256473\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  325 \n",
      "-----------------------------------\n",
      "Training Loss  0.5277672785051754\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5431289452573528\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  326 \n",
      "-----------------------------------\n",
      "Training Loss  0.5277526723534873\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5431327457013337\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  327 \n",
      "-----------------------------------\n",
      "Training Loss  0.5277538624372375\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5431892845941626\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  328 \n",
      "-----------------------------------\n",
      "Training Loss  0.5277526201157088\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5431852923787158\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  329 \n",
      "-----------------------------------\n",
      "Training Loss  0.5277407661582647\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5432784971983536\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  330 \n",
      "-----------------------------------\n",
      "Training Loss  0.5277453650919239\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5432699504105941\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  331 \n",
      "-----------------------------------\n",
      "Training Loss  0.5277175012599217\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5432751489722211\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  332 \n",
      "-----------------------------------\n",
      "Training Loss  0.5277302030766948\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5432920974233876\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  333 \n",
      "-----------------------------------\n",
      "Training Loss  0.5277162894104304\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5432764084442802\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  334 \n",
      "-----------------------------------\n",
      "Training Loss  0.5277210396997044\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543336614318516\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  335 \n",
      "-----------------------------------\n",
      "Training Loss  0.5277244696456395\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433105100756106\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  336 \n",
      "-----------------------------------\n",
      "Training Loss  0.527696049950096\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433041686597078\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  337 \n",
      "-----------------------------------\n",
      "Training Loss  0.5277165753787822\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433586421220199\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  338 \n",
      "-----------------------------------\n",
      "Training Loss  0.5276941121294257\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433613735696544\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  339 \n",
      "-----------------------------------\n",
      "Training Loss  0.5276963278818666\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433216691017151\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  340 \n",
      "-----------------------------------\n",
      "Training Loss  0.52769314740481\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433483382929927\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  341 \n",
      "-----------------------------------\n",
      "Training Loss  0.5277066311139739\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433697830075803\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  342 \n",
      "-----------------------------------\n",
      "Training Loss  0.527675562695171\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5432944751304128\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  343 \n",
      "-----------------------------------\n",
      "Training Loss  0.5276755971854992\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433191449745841\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  344 \n",
      "-----------------------------------\n",
      "Training Loss  0.5276746954141038\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433273121066715\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  345 \n",
      "-----------------------------------\n",
      "Training Loss  0.5276654875010587\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433257546113885\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  346 \n",
      "-----------------------------------\n",
      "Training Loss  0.527671464708414\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433339411797731\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  347 \n",
      "-----------------------------------\n",
      "Training Loss  0.5276606220207857\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433300085689711\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  348 \n",
      "-----------------------------------\n",
      "Training Loss  0.5276544057251362\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433015253232873\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  349 \n",
      "-----------------------------------\n",
      "Training Loss  0.5276424338978328\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5432949558548306\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  350 \n",
      "-----------------------------------\n",
      "Training Loss  0.5276420970311325\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5432948586733445\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  351 \n",
      "-----------------------------------\n",
      "Training Loss  0.5276483076342037\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5432959807955701\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  352 \n",
      "-----------------------------------\n",
      "Training Loss  0.5276365454277295\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433268896911455\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  353 \n",
      "-----------------------------------\n",
      "Training Loss  0.5276294906487625\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433186124200406\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  354 \n",
      "-----------------------------------\n",
      "Training Loss  0.5276130759314205\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5432475299938865\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  355 \n",
      "-----------------------------------\n",
      "Training Loss  0.5276203731472573\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543242008789726\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  356 \n",
      "-----------------------------------\n",
      "Training Loss  0.5276188261053535\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5432744855466096\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  357 \n",
      "-----------------------------------\n",
      "Training Loss  0.5276013435272688\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5432719432789347\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  358 \n",
      "-----------------------------------\n",
      "Training Loss  0.5276075665200695\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5432902198770772\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  359 \n",
      "-----------------------------------\n",
      "Training Loss  0.5276069403364417\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433098945928656\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  360 \n",
      "-----------------------------------\n",
      "Training Loss  0.5275838863983583\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5432768515918566\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  361 \n",
      "-----------------------------------\n",
      "Training Loss  0.5275904475972893\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543297625106314\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  362 \n",
      "-----------------------------------\n",
      "Training Loss  0.5275729764043615\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433210924915646\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  363 \n",
      "-----------------------------------\n",
      "Training Loss  0.5275818645284417\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433491196321405\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  364 \n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss  0.5275889993383643\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543373659900997\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  365 \n",
      "-----------------------------------\n",
      "Training Loss  0.5275649535522032\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543359161719032\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  366 \n",
      "-----------------------------------\n",
      "Training Loss  0.5275735295890422\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433841166288956\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  367 \n",
      "-----------------------------------\n",
      "Training Loss  0.5275676116514741\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434172917967257\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  368 \n",
      "-----------------------------------\n",
      "Training Loss  0.5275702724296055\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434209574823794\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  369 \n",
      "-----------------------------------\n",
      "Training Loss  0.5275614308507255\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434286296367645\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  370 \n",
      "-----------------------------------\n",
      "Training Loss  0.5275571483574556\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543451269035754\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  371 \n",
      "-----------------------------------\n",
      "Training Loss  0.5275482870219799\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434114842311196\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  372 \n",
      "-----------------------------------\n",
      "Training Loss  0.527543239379197\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434676110744476\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  373 \n",
      "-----------------------------------\n",
      "Training Loss  0.5275502047511968\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434408757997595\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  374 \n",
      "-----------------------------------\n",
      "Training Loss  0.5275515786717447\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434888057086779\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  375 \n",
      "-----------------------------------\n",
      "Training Loss  0.5275371141648024\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543482129988463\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  376 \n",
      "-----------------------------------\n",
      "Training Loss  0.5275369269794292\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434986469538315\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  377 \n",
      "-----------------------------------\n",
      "Training Loss  0.5275304880035058\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5435155617154163\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  378 \n",
      "-----------------------------------\n",
      "Training Loss  0.5275298427329974\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5435404233310533\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  379 \n",
      "-----------------------------------\n",
      "Training Loss  0.5275168914473458\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5435574534146682\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  380 \n",
      "-----------------------------------\n",
      "Training Loss  0.5275167283717166\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5435908916203872\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  381 \n",
      "-----------------------------------\n",
      "Training Loss  0.5275010777993149\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5435965488786283\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  382 \n",
      "-----------------------------------\n",
      "Training Loss  0.5275144419643316\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5436199799827908\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  383 \n",
      "-----------------------------------\n",
      "Training Loss  0.5274908941113547\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5435689260130343\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  384 \n",
      "-----------------------------------\n",
      "Training Loss  0.5274888410996855\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5435646007890287\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  385 \n",
      "-----------------------------------\n",
      "Training Loss  0.5274894542238685\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.54356540415598\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  386 \n",
      "-----------------------------------\n",
      "Training Loss  0.5274815535947178\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5435248483782229\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  387 \n",
      "-----------------------------------\n",
      "Training Loss  0.5274829492810067\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5435435499833978\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  388 \n",
      "-----------------------------------\n",
      "Training Loss  0.5274658949857347\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5435192222180574\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  389 \n",
      "-----------------------------------\n",
      "Training Loss  0.5274582886963748\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434896259204202\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  390 \n",
      "-----------------------------------\n",
      "Training Loss  0.5274500143661928\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434535119844519\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  391 \n",
      "-----------------------------------\n",
      "Training Loss  0.5274522160546163\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434287760568701\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  392 \n",
      "-----------------------------------\n",
      "Training Loss  0.5274480186151654\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433957991392716\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  393 \n",
      "-----------------------------------\n",
      "Training Loss  0.527442037724377\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433807917263197\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  394 \n",
      "-----------------------------------\n",
      "Training Loss  0.5274273535508788\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433719572813615\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  395 \n",
      "-----------------------------------\n",
      "Training Loss  0.5274267163169518\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433678082797838\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  396 \n",
      "-----------------------------------\n",
      "Training Loss  0.5274266757991877\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433693903943767\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  397 \n",
      "-----------------------------------\n",
      "Training Loss  0.5274162520183606\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.54335643545441\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  398 \n",
      "-----------------------------------\n",
      "Training Loss  0.527417791023683\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433535096438035\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  399 \n",
      "-----------------------------------\n",
      "Training Loss  0.527406382426787\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433757590210956\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  400 \n",
      "-----------------------------------\n",
      "Training Loss  0.527412949653154\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433820589728977\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  401 \n",
      "-----------------------------------\n",
      "Training Loss  0.5274047560236427\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433840635030166\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  402 \n",
      "-----------------------------------\n",
      "Training Loss  0.5273926884270785\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433897207612577\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  403 \n",
      "-----------------------------------\n",
      "Training Loss  0.5273974263266231\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433973980986554\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  404 \n",
      "-----------------------------------\n",
      "Training Loss  0.5273903337087524\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433995995832526\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  405 \n",
      "-----------------------------------\n",
      "Training Loss  0.5273945971151416\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433802306652069\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  406 \n",
      "-----------------------------------\n",
      "Training Loss  0.5273831865091002\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434018658555072\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  407 \n",
      "-----------------------------------\n",
      "Training Loss  0.5273919262912836\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434257465860118\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  408 \n",
      "-----------------------------------\n",
      "Training Loss  0.5273779664146766\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433687736158785\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  409 \n",
      "-----------------------------------\n",
      "Training Loss  0.527379578418946\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434001360250555\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  410 \n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss  0.5273757617125351\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434163588544597\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  411 \n",
      "-----------------------------------\n",
      "Training Loss  0.5273788765575109\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434313688589179\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  412 \n",
      "-----------------------------------\n",
      "Training Loss  0.5273759971173961\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434322447880454\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  413 \n",
      "-----------------------------------\n",
      "Training Loss  0.5273722380064847\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434315671091494\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  414 \n",
      "-----------------------------------\n",
      "Training Loss  0.5273812282621191\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434526464213496\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  415 \n",
      "-----------------------------------\n",
      "Training Loss  0.5273601121447059\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434507844240769\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  416 \n",
      "-----------------------------------\n",
      "Training Loss  0.5273654353752565\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434479894845382\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  417 \n",
      "-----------------------------------\n",
      "Training Loss  0.5273655927583073\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543446790912877\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  418 \n",
      "-----------------------------------\n",
      "Training Loss  0.527352792493413\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543437624755113\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  419 \n",
      "-----------------------------------\n",
      "Training Loss  0.5273638635538938\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434322590413301\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  420 \n",
      "-----------------------------------\n",
      "Training Loss  0.527365569653136\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434285518915757\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  421 \n",
      "-----------------------------------\n",
      "Training Loss  0.5273509534557214\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434274232905844\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  422 \n",
      "-----------------------------------\n",
      "Training Loss  0.527352295899659\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434284417525582\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  423 \n",
      "-----------------------------------\n",
      "Training Loss  0.5273419109623084\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434331103511478\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  424 \n",
      "-----------------------------------\n",
      "Training Loss  0.527350999666064\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434228855630626\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  425 \n",
      "-----------------------------------\n",
      "Training Loss  0.5273464392410235\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434027897275012\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  426 \n",
      "-----------------------------------\n",
      "Training Loss  0.5273400833097737\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434017608995023\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  427 \n",
      "-----------------------------------\n",
      "Training Loss  0.527344576093588\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434236643107041\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  428 \n",
      "-----------------------------------\n",
      "Training Loss  0.527336277653662\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434221962223882\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  429 \n",
      "-----------------------------------\n",
      "Training Loss  0.5273386685366042\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434169782244641\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  430 \n",
      "-----------------------------------\n",
      "Training Loss  0.5273309865694368\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434083705363066\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  431 \n",
      "-----------------------------------\n",
      "Training Loss  0.5273298972778107\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434229542379794\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  432 \n",
      "-----------------------------------\n",
      "Training Loss  0.5273291857055064\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434039468350618\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  433 \n",
      "-----------------------------------\n",
      "Training Loss  0.5273240299037333\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433976702068163\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  434 \n",
      "-----------------------------------\n",
      "Training Loss  0.5273361313209105\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434424060842266\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  435 \n",
      "-----------------------------------\n",
      "Training Loss  0.5273217863581153\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434120206729226\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  436 \n",
      "-----------------------------------\n",
      "Training Loss  0.5273153822073776\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434191278789354\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  437 \n",
      "-----------------------------------\n",
      "Training Loss  0.5273188972071315\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543406614790792\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  438 \n",
      "-----------------------------------\n",
      "Training Loss  0.5273140856389249\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433631694835165\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  439 \n",
      "-----------------------------------\n",
      "Training Loss  0.5273084620411477\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543368927810503\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  440 \n",
      "-----------------------------------\n",
      "Training Loss  0.5273056294810906\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543383345655773\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  441 \n",
      "-----------------------------------\n",
      "Training Loss  0.5273046393073006\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433708247931107\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  442 \n",
      "-----------------------------------\n",
      "Training Loss  0.5273073539974984\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433830035769421\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  443 \n",
      "-----------------------------------\n",
      "Training Loss  0.5273008838798223\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433849925580232\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  444 \n",
      "-----------------------------------\n",
      "Training Loss  0.5272996224714129\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433880220288816\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  445 \n",
      "-----------------------------------\n",
      "Training Loss  0.5272915524043394\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433914855770443\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  446 \n",
      "-----------------------------------\n",
      "Training Loss  0.5272878629437993\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434110773646313\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  447 \n",
      "-----------------------------------\n",
      "Training Loss  0.5272838620657332\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433898684771165\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  448 \n",
      "-----------------------------------\n",
      "Training Loss  0.5272866239708461\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433929238630377\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  449 \n",
      "-----------------------------------\n",
      "Training Loss  0.5272824968514818\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433942779250767\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  450 \n",
      "-----------------------------------\n",
      "Training Loss  0.5272749491621939\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433992458426434\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  451 \n",
      "-----------------------------------\n",
      "Training Loss  0.5272703334856569\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433566648027172\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  452 \n",
      "-----------------------------------\n",
      "Training Loss  0.5272723195258151\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433415653912917\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  453 \n",
      "-----------------------------------\n",
      "Training Loss  0.527267681413822\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433514364387678\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  454 \n",
      "-----------------------------------\n",
      "Training Loss  0.5272741686092334\n",
      "Training Accuracy  0.5990081473609635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss  0.5433945902015852\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  455 \n",
      "-----------------------------------\n",
      "Training Loss  0.5272650839237685\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543403324873551\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  456 \n",
      "-----------------------------------\n",
      "Training Loss  0.5272575211658906\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543370070664779\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  457 \n",
      "-----------------------------------\n",
      "Training Loss  0.527265057135164\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543405972097231\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  458 \n",
      "-----------------------------------\n",
      "Training Loss  0.5272626568762104\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433873080688975\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  459 \n",
      "-----------------------------------\n",
      "Training Loss  0.5272515104727798\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433902131474536\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  460 \n",
      "-----------------------------------\n",
      "Training Loss  0.5272581349597888\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434213384338047\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  461 \n",
      "-----------------------------------\n",
      "Training Loss  0.5272474851501122\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434058788030044\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  462 \n",
      "-----------------------------------\n",
      "Training Loss  0.5272517532445071\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434079986551533\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  463 \n",
      "-----------------------------------\n",
      "Training Loss  0.5272440173652735\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434031965939895\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  464 \n",
      "-----------------------------------\n",
      "Training Loss  0.5272442718570152\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434543814348138\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  465 \n",
      "-----------------------------------\n",
      "Training Loss  0.5272488811712587\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434546574302341\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  466 \n",
      "-----------------------------------\n",
      "Training Loss  0.5272441101208162\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434231226858885\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  467 \n",
      "-----------------------------------\n",
      "Training Loss  0.5272560320543439\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434558495231296\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  468 \n",
      "-----------------------------------\n",
      "Training Loss  0.5272391602564394\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434389282827792\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  469 \n",
      "-----------------------------------\n",
      "Training Loss  0.5272364991434505\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434305175491001\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  470 \n",
      "-----------------------------------\n",
      "Training Loss  0.5272521292895413\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434488796669504\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  471 \n",
      "-----------------------------------\n",
      "Training Loss  0.5272268170721075\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434458903644396\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  472 \n",
      "-----------------------------------\n",
      "Training Loss  0.5272297139248151\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434388725653939\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  473 \n",
      "-----------------------------------\n",
      "Training Loss  0.527243916908007\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434828724550165\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  474 \n",
      "-----------------------------------\n",
      "Training Loss  0.5272305644630046\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434412476809128\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  475 \n",
      "-----------------------------------\n",
      "Training Loss  0.527223575650976\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434063685976941\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  476 \n",
      "-----------------------------------\n",
      "Training Loss  0.5272312552741404\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434419473876124\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  477 \n",
      "-----------------------------------\n",
      "Training Loss  0.5272366448064868\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5435031315554744\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  478 \n",
      "-----------------------------------\n",
      "Training Loss  0.5272110349006867\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434532904106638\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  479 \n",
      "-----------------------------------\n",
      "Training Loss  0.5272261091832364\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434475320836772\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  480 \n",
      "-----------------------------------\n",
      "Training Loss  0.5272297973043463\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434538281482199\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  481 \n",
      "-----------------------------------\n",
      "Training Loss  0.5272177849592787\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434463037096936\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  482 \n",
      "-----------------------------------\n",
      "Training Loss  0.5272113731068172\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434328848901002\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  483 \n",
      "-----------------------------------\n",
      "Training Loss  0.5272194324584489\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434751251469487\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  484 \n",
      "-----------------------------------\n",
      "Training Loss  0.5272092226515995\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543449033861575\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  485 \n",
      "-----------------------------------\n",
      "Training Loss  0.527217710286044\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434214848539104\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  486 \n",
      "-----------------------------------\n",
      "Training Loss  0.527207192410244\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434019228686457\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  487 \n",
      "-----------------------------------\n",
      "Training Loss  0.527207786782404\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433745474919028\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  488 \n",
      "-----------------------------------\n",
      "Training Loss  0.5271959452146895\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433911318364351\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  489 \n",
      "-----------------------------------\n",
      "Training Loss  0.5271975317697847\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433918587539507\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  490 \n",
      "-----------------------------------\n",
      "Training Loss  0.5272034731473816\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433437642843827\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  491 \n",
      "-----------------------------------\n",
      "Training Loss  0.5271924928332983\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5432985437952954\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  492 \n",
      "-----------------------------------\n",
      "Training Loss  0.5271916600425591\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5432749494262363\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  493 \n",
      "-----------------------------------\n",
      "Training Loss  0.5271938041354833\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5432315105977266\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  494 \n",
      "-----------------------------------\n",
      "Training Loss  0.5271749174996708\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5432342640731646\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  495 \n",
      "-----------------------------------\n",
      "Training Loss  0.5271765335222308\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5432244008002074\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  496 \n",
      "-----------------------------------\n",
      "Training Loss  0.52717000379991\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543181660382644\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  497 \n",
      "-----------------------------------\n",
      "Training Loss  0.5271490631478556\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5431173482666845\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  498 \n",
      "-----------------------------------\n",
      "Training Loss  0.5271764528215601\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543128920638043\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  499 \n",
      "-----------------------------------\n",
      "Training Loss  0.5271572698368115\n",
      "Training Accuracy  0.5990081473609635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss  0.5430805048216945\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  500 \n",
      "-----------------------------------\n",
      "Training Loss  0.5271614759825589\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543028826298921\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  501 \n",
      "-----------------------------------\n",
      "Training Loss  0.5271544523453444\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5430449065954789\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  502 \n",
      "-----------------------------------\n",
      "Training Loss  0.5271420887347018\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5429785795833754\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  503 \n",
      "-----------------------------------\n",
      "Training Loss  0.5271518632267298\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5430250945298568\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  504 \n",
      "-----------------------------------\n",
      "Training Loss  0.5271422829520837\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5430147907008296\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  505 \n",
      "-----------------------------------\n",
      "Training Loss  0.5271476011597709\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5429943540821904\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  506 \n",
      "-----------------------------------\n",
      "Training Loss  0.527121377125215\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5429798947728198\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  507 \n",
      "-----------------------------------\n",
      "Training Loss  0.5271450659532225\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5430041979188505\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  508 \n",
      "-----------------------------------\n",
      "Training Loss  0.5271390023526181\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5429873440576636\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  509 \n",
      "-----------------------------------\n",
      "Training Loss  0.5271271437071683\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5429980664149575\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  510 \n",
      "-----------------------------------\n",
      "Training Loss  0.5271175584096587\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5429767914440321\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  511 \n",
      "-----------------------------------\n",
      "Training Loss  0.5271299776066555\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5429701520049054\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  512 \n",
      "-----------------------------------\n",
      "Training Loss  0.5271313220597384\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5429594231688458\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  513 \n",
      "-----------------------------------\n",
      "Training Loss  0.5271182790231169\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5429921176122583\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  514 \n",
      "-----------------------------------\n",
      "Training Loss  0.5271143521485704\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5429497957229614\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  515 \n",
      "-----------------------------------\n",
      "Training Loss  0.5271035714095897\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5428914000158724\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  516 \n",
      "-----------------------------------\n",
      "Training Loss  0.5271142148569729\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5429464630458666\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  517 \n",
      "-----------------------------------\n",
      "Training Loss  0.5271036129319266\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5429761941018312\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  518 \n",
      "-----------------------------------\n",
      "Training Loss  0.5270947361930033\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5430157586284305\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  519 \n",
      "-----------------------------------\n",
      "Training Loss  0.5270976678708966\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.542987056400465\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  520 \n",
      "-----------------------------------\n",
      "Training Loss  0.5270962674966019\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5430699029694432\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  521 \n",
      "-----------------------------------\n",
      "Training Loss  0.5270841094884979\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5430110239464304\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  522 \n",
      "-----------------------------------\n",
      "Training Loss  0.5270903324812986\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543069628269776\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  523 \n",
      "-----------------------------------\n",
      "Training Loss  0.5270781748080522\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5430167058239812\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  524 \n",
      "-----------------------------------\n",
      "Training Loss  0.5270813891057218\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5430742489254993\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  525 \n",
      "-----------------------------------\n",
      "Training Loss  0.5270761137598017\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5430416166782379\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  526 \n",
      "-----------------------------------\n",
      "Training Loss  0.5270720837491282\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543060258678768\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  527 \n",
      "-----------------------------------\n",
      "Training Loss  0.5270759597253264\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5430709862190745\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  528 \n",
      "-----------------------------------\n",
      "Training Loss  0.5270778459779332\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5430764763251595\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  529 \n",
      "-----------------------------------\n",
      "Training Loss  0.5270753656880239\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543054153089938\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  530 \n",
      "-----------------------------------\n",
      "Training Loss  0.5270711890097415\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5431406135144441\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  531 \n",
      "-----------------------------------\n",
      "Training Loss  0.5270525474896591\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5430292189121246\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  532 \n",
      "-----------------------------------\n",
      "Training Loss  0.5270690288436547\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5431173767732538\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  533 \n",
      "-----------------------------------\n",
      "Training Loss  0.527056717135933\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5431129815785781\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  534 \n",
      "-----------------------------------\n",
      "Training Loss  0.5270562888531203\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5431235950926075\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  535 \n",
      "-----------------------------------\n",
      "Training Loss  0.5270538899335968\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5431225792221401\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  536 \n",
      "-----------------------------------\n",
      "Training Loss  0.5270578409178873\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5431544508623041\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  537 \n",
      "-----------------------------------\n",
      "Training Loss  0.5270447935281175\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5431617679803268\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  538 \n",
      "-----------------------------------\n",
      "Training Loss  0.5270361953906799\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.54320214105689\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  539 \n",
      "-----------------------------------\n",
      "Training Loss  0.5270239375950245\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5432209605756013\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  540 \n",
      "-----------------------------------\n",
      "Training Loss  0.5270487994290469\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5432367260041444\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  541 \n",
      "-----------------------------------\n",
      "Training Loss  0.5270202652122198\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5432517152765522\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  542 \n",
      "-----------------------------------\n",
      "Training Loss  0.5270276675733288\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433374954306561\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  543 \n",
      "-----------------------------------\n",
      "Training Loss  0.5270228134782127\n",
      "Training Accuracy  0.5990081473609635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss  0.543371786241946\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  544 \n",
      "-----------------------------------\n",
      "Training Loss  0.5270158193084631\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433859462323396\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  545 \n",
      "-----------------------------------\n",
      "Training Loss  0.5270241331518366\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434531997079435\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  546 \n",
      "-----------------------------------\n",
      "Training Loss  0.5270227565524284\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5435075967208199\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  547 \n",
      "-----------------------------------\n",
      "Training Loss  0.5270167759964975\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434839803239574\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  548 \n",
      "-----------------------------------\n",
      "Training Loss  0.527021267106024\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434619823227758\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  549 \n",
      "-----------------------------------\n",
      "Training Loss  0.5270075513405746\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434528861356818\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  550 \n",
      "-----------------------------------\n",
      "Training Loss  0.5270156281047993\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434929598932681\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  551 \n",
      "-----------------------------------\n",
      "Training Loss  0.5270099857549989\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434731270955957\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  552 \n",
      "-----------------------------------\n",
      "Training Loss  0.527016403969754\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5435266909391984\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  553 \n",
      "-----------------------------------\n",
      "Training Loss  0.5270056563816713\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5435073103593744\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  554 \n",
      "-----------------------------------\n",
      "Training Loss  0.5269882109727753\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434600075949794\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  555 \n",
      "-----------------------------------\n",
      "Training Loss  0.5270006479172225\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434869320496268\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  556 \n",
      "-----------------------------------\n",
      "Training Loss  0.5270032403844126\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434798935185308\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  557 \n",
      "-----------------------------------\n",
      "Training Loss  0.5269979241858708\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434785601885422\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  558 \n",
      "-----------------------------------\n",
      "Training Loss  0.5270137368293291\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5435262555661409\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  559 \n",
      "-----------------------------------\n",
      "Training Loss  0.5269956401224887\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5435021999089614\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  560 \n",
      "-----------------------------------\n",
      "Training Loss  0.5269876765401176\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434904681599658\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  561 \n",
      "-----------------------------------\n",
      "Training Loss  0.5269971406191922\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5435238908166471\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  562 \n",
      "-----------------------------------\n",
      "Training Loss  0.5269820134291489\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5435207032639048\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  563 \n",
      "-----------------------------------\n",
      "Training Loss  0.526988673076201\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434991484102996\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  564 \n",
      "-----------------------------------\n",
      "Training Loss  0.5269876802235507\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5435257981652799\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  565 \n",
      "-----------------------------------\n",
      "Training Loss  0.5269838859525959\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5435652292293051\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  566 \n",
      "-----------------------------------\n",
      "Training Loss  0.5269832835438546\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5435360631217128\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  567 \n",
      "-----------------------------------\n",
      "Training Loss  0.5269654795025172\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543492493422135\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  568 \n",
      "-----------------------------------\n",
      "Training Loss  0.5269870537050655\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5435328911180082\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  569 \n",
      "-----------------------------------\n",
      "Training Loss  0.526978023601382\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5435793633046357\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  570 \n",
      "-----------------------------------\n",
      "Training Loss  0.5269705254710122\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543566072764604\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  571 \n",
      "-----------------------------------\n",
      "Training Loss  0.5269745933205894\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5435982605685359\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  572 \n",
      "-----------------------------------\n",
      "Training Loss  0.5269501065270285\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5436300648295361\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  573 \n",
      "-----------------------------------\n",
      "Training Loss  0.5269746204440513\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5436950675819231\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  574 \n",
      "-----------------------------------\n",
      "Training Loss  0.5269612562790346\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5437218883763189\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  575 \n",
      "-----------------------------------\n",
      "Training Loss  0.5269630343726511\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5436913409958715\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  576 \n",
      "-----------------------------------\n",
      "Training Loss  0.5269619762227776\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5437730058379795\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  577 \n",
      "-----------------------------------\n",
      "Training Loss  0.5269509925601188\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5437349625255751\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  578 \n",
      "-----------------------------------\n",
      "Training Loss  0.5269528962253185\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5437382952026699\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  579 \n",
      "-----------------------------------\n",
      "Training Loss  0.5269469735997446\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.54380086193914\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  580 \n",
      "-----------------------------------\n",
      "Training Loss  0.5269630973258715\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5437745089116304\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  581 \n",
      "-----------------------------------\n",
      "Training Loss  0.5269360465280125\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5437687402186187\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  582 \n",
      "-----------------------------------\n",
      "Training Loss  0.5269347000657842\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5437662653301073\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  583 \n",
      "-----------------------------------\n",
      "Training Loss  0.5269433678535933\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5438186772491621\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  584 \n",
      "-----------------------------------\n",
      "Training Loss  0.5269350114833103\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5437930381816366\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  585 \n",
      "-----------------------------------\n",
      "Training Loss  0.5269446463397379\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5437744868838269\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  586 \n",
      "-----------------------------------\n",
      "Training Loss  0.5269310504532931\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5437852843948032\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  587 \n",
      "-----------------------------------\n",
      "Training Loss  0.5269253066416537\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5437925328379092\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  588 \n",
      "-----------------------------------\n",
      "Training Loss  0.5269330364934514\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5437791477078977\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  589 \n",
      "-----------------------------------\n",
      "Training Loss  0.5269193773189288\n",
      "Training Accuracy  0.5990081473609635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss  0.5437757774539616\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  590 \n",
      "-----------------------------------\n",
      "Training Loss  0.5269398927018883\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5438358317250791\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  591 \n",
      "-----------------------------------\n",
      "Training Loss  0.5269112931878379\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5437989584777666\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  592 \n",
      "-----------------------------------\n",
      "Training Loss  0.5269299956519952\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5438504206097644\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  593 \n",
      "-----------------------------------\n",
      "Training Loss  0.5269104024667418\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543826357178066\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  594 \n",
      "-----------------------------------\n",
      "Training Loss  0.5269265496328975\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5438515310702117\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  595 \n",
      "-----------------------------------\n",
      "Training Loss  0.5269211219267899\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5439016624637272\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  596 \n",
      "-----------------------------------\n",
      "Training Loss  0.5269100702880474\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5439051389694214\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  597 \n",
      "-----------------------------------\n",
      "Training Loss  0.5269146377450964\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5439721397731615\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  598 \n",
      "-----------------------------------\n",
      "Training Loss  0.5268935785534676\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5439709489760192\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  599 \n",
      "-----------------------------------\n",
      "Training Loss  0.5268982407752048\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5440055352190266\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  600 \n",
      "-----------------------------------\n",
      "Training Loss  0.5268844747811221\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5440547582895859\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  601 \n",
      "-----------------------------------\n",
      "Training Loss  0.5268695464964663\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5441209933032161\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  602 \n",
      "-----------------------------------\n",
      "Training Loss  0.5268678169571952\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5442045654939569\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  603 \n",
      "-----------------------------------\n",
      "Training Loss  0.52685309126136\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5442394277323848\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  604 \n",
      "-----------------------------------\n",
      "Training Loss  0.5268409292349655\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5442417989606443\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  605 \n",
      "-----------------------------------\n",
      "Training Loss  0.526846053894986\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.544289275355961\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  606 \n",
      "-----------------------------------\n",
      "Training Loss  0.5268274334709296\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5443319626476454\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  607 \n",
      "-----------------------------------\n",
      "Training Loss  0.526827015568701\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5443246325720912\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  608 \n",
      "-----------------------------------\n",
      "Training Loss  0.5268195418829329\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5443643759126249\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  609 \n",
      "-----------------------------------\n",
      "Training Loss  0.5268059399690521\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5443726143111354\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  610 \n",
      "-----------------------------------\n",
      "Training Loss  0.5268136658025592\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5444193313951078\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  611 \n",
      "-----------------------------------\n",
      "Training Loss  0.5267915253558856\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5444362370864205\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  612 \n",
      "-----------------------------------\n",
      "Training Loss  0.5267995790149389\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5444351719773334\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  613 \n",
      "-----------------------------------\n",
      "Training Loss  0.526782223682725\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5444570546564849\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  614 \n",
      "-----------------------------------\n",
      "Training Loss  0.5267613145072808\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5444125442401223\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  615 \n",
      "-----------------------------------\n",
      "Training Loss  0.5267783296242189\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.544476603684218\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  616 \n",
      "-----------------------------------\n",
      "Training Loss  0.5267661528640919\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5445050078889598\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  617 \n",
      "-----------------------------------\n",
      "Training Loss  0.526765412494038\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5445694884528285\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  618 \n",
      "-----------------------------------\n",
      "Training Loss  0.5267585184466973\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5445884103360383\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  619 \n",
      "-----------------------------------\n",
      "Training Loss  0.5267555499344729\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5446373496366583\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  620 \n",
      "-----------------------------------\n",
      "Training Loss  0.5267602084727769\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5447380361349686\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  621 \n",
      "-----------------------------------\n",
      "Training Loss  0.5267406243286775\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5447813972182896\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  622 \n",
      "-----------------------------------\n",
      "Training Loss  0.5267378075069256\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5448412623094476\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  623 \n",
      "-----------------------------------\n",
      "Training Loss  0.5267333816946222\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5448342950447745\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  624 \n",
      "-----------------------------------\n",
      "Training Loss  0.5267234019349131\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5448640040729357\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  625 \n",
      "-----------------------------------\n",
      "Training Loss  0.5267296935735124\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.544829811738885\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  626 \n",
      "-----------------------------------\n",
      "Training Loss  0.5267184293002225\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5448068198950394\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  627 \n",
      "-----------------------------------\n",
      "Training Loss  0.5267095314652732\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5447934632715972\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  628 \n",
      "-----------------------------------\n",
      "Training Loss  0.5267121517256405\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5447405395300492\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  629 \n",
      "-----------------------------------\n",
      "Training Loss  0.5266931512382593\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.544766226540441\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  630 \n",
      "-----------------------------------\n",
      "Training Loss  0.5266926251770405\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5447024275427279\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  631 \n",
      "-----------------------------------\n",
      "Training Loss  0.5266661694210567\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5446183098399121\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  632 \n",
      "-----------------------------------\n",
      "Training Loss  0.5266857100336739\n",
      "Training Accuracy  0.5990081473609635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss  0.5446636093699414\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  633 \n",
      "-----------------------------------\n",
      "Training Loss  0.5266737683435504\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5445778927077418\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  634 \n",
      "-----------------------------------\n",
      "Training Loss  0.5266499556182476\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5445612668991089\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  635 \n",
      "-----------------------------------\n",
      "Training Loss  0.5266353879751784\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5444772761801014\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  636 \n",
      "-----------------------------------\n",
      "Training Loss  0.5266217284657982\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5443743609863779\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  637 \n",
      "-----------------------------------\n",
      "Training Loss  0.5266227815928084\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5444210560425468\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  638 \n",
      "-----------------------------------\n",
      "Training Loss  0.5266017578960804\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5443367465682651\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  639 \n",
      "-----------------------------------\n",
      "Training Loss  0.5266157890973466\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.544329936089723\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  640 \n",
      "-----------------------------------\n",
      "Training Loss  0.5265987883792834\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5443773464016293\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  641 \n",
      "-----------------------------------\n",
      "Training Loss  0.5266005363357201\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5443154353162517\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  642 \n",
      "-----------------------------------\n",
      "Training Loss  0.5265849410148149\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5442718513633894\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  643 \n",
      "-----------------------------------\n",
      "Training Loss  0.5265721963362747\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5442776174648948\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  644 \n",
      "-----------------------------------\n",
      "Training Loss  0.526578377471881\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5442058612471041\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  645 \n",
      "-----------------------------------\n",
      "Training Loss  0.526572203703141\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5441829924998076\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  646 \n",
      "-----------------------------------\n",
      "Training Loss  0.526564451415887\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5441614311674366\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  647 \n",
      "-----------------------------------\n",
      "Training Loss  0.5265657325808921\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5441876688729161\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  648 \n",
      "-----------------------------------\n",
      "Training Loss  0.5265399009993906\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5441629173962966\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  649 \n",
      "-----------------------------------\n",
      "Training Loss  0.5265475293893492\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5441442818745322\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  650 \n",
      "-----------------------------------\n",
      "Training Loss  0.5265555763512515\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5442152243593464\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  651 \n",
      "-----------------------------------\n",
      "Training Loss  0.526536418815677\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5441793903060581\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  652 \n",
      "-----------------------------------\n",
      "Training Loss  0.5265343899137518\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.54417833297149\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  653 \n",
      "-----------------------------------\n",
      "Training Loss  0.5265662479266692\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5442239940166473\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  654 \n",
      "-----------------------------------\n",
      "Training Loss  0.5265380381868127\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5441973157550978\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  655 \n",
      "-----------------------------------\n",
      "Training Loss  0.5265503884031532\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5442220983297928\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  656 \n",
      "-----------------------------------\n",
      "Training Loss  0.5265025144882416\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5441759708135024\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  657 \n",
      "-----------------------------------\n",
      "Training Loss  0.5265053457088684\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5441533573295759\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  658 \n",
      "-----------------------------------\n",
      "Training Loss  0.5264960852231872\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5441968816777935\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  659 \n",
      "-----------------------------------\n",
      "Training Loss  0.5264579007464848\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5441204892552417\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  660 \n",
      "-----------------------------------\n",
      "Training Loss  0.5264510066991441\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5441410761812459\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  661 \n",
      "-----------------------------------\n",
      "Training Loss  0.5264394557877873\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5440983461297076\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  662 \n",
      "-----------------------------------\n",
      "Training Loss  0.5264280016502637\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5441119891145955\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  663 \n",
      "-----------------------------------\n",
      "Training Loss  0.5263771592231279\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5440621777721073\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  664 \n",
      "-----------------------------------\n",
      "Training Loss  0.5263524490795778\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5441184264162312\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  665 \n",
      "-----------------------------------\n",
      "Training Loss  0.5263301897584722\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5441810890384342\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  666 \n",
      "-----------------------------------\n",
      "Training Loss  0.5262868186731017\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5441384004509967\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  667 \n",
      "-----------------------------------\n",
      "Training Loss  0.5263097423515962\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5441501347914987\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  668 \n",
      "-----------------------------------\n",
      "Training Loss  0.526312481151538\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5441855088524197\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  669 \n",
      "-----------------------------------\n",
      "Training Loss  0.5262825783718837\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5441193839778071\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  670 \n",
      "-----------------------------------\n",
      "Training Loss  0.5262916141681457\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5441687910453133\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  671 \n",
      "-----------------------------------\n",
      "Training Loss  0.526293985294492\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5441871052202971\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  672 \n",
      "-----------------------------------\n",
      "Training Loss  0.526281933436233\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.544139703978663\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  673 \n",
      "-----------------------------------\n",
      "Training Loss  0.5262486398889777\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5441574907821157\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  674 \n",
      "-----------------------------------\n",
      "Training Loss  0.5262403059541509\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5441018731697745\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  675 \n",
      "-----------------------------------\n",
      "Training Loss  0.5262428820133209\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5440330414668374\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  676 \n",
      "-----------------------------------\n",
      "Training Loss  0.5262266260184599\n",
      "Training Accuracy  0.5990081473609635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss  0.5439721410689147\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  677 \n",
      "-----------------------------------\n",
      "Training Loss  0.5262413021553768\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5440258655859076\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  678 \n",
      "-----------------------------------\n",
      "Training Loss  0.5262017879593238\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5439342234445654\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  679 \n",
      "-----------------------------------\n",
      "Training Loss  0.5262033333269398\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543940307005592\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  680 \n",
      "-----------------------------------\n",
      "Training Loss  0.5262101366278831\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5439284936241482\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  681 \n",
      "-----------------------------------\n",
      "Training Loss  0.5261991864509796\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5438799352749534\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  682 \n",
      "-----------------------------------\n",
      "Training Loss  0.5261971682644961\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5439066083534904\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  683 \n",
      "-----------------------------------\n",
      "Training Loss  0.5261873924330379\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543856768504433\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  684 \n",
      "-----------------------------------\n",
      "Training Loss  0.5261703036474378\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5438061810058096\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  685 \n",
      "-----------------------------------\n",
      "Training Loss  0.5261811419819178\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5438272058963776\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  686 \n",
      "-----------------------------------\n",
      "Training Loss  0.526134458485614\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543790279523186\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  687 \n",
      "-----------------------------------\n",
      "Training Loss  0.5261100339755583\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5436504405477772\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  688 \n",
      "-----------------------------------\n",
      "Training Loss  0.5261151502641399\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5436554602954699\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  689 \n",
      "-----------------------------------\n",
      "Training Loss  0.526094444012374\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5435902722503828\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  690 \n",
      "-----------------------------------\n",
      "Training Loss  0.5261018745015177\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5435580118842747\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  691 \n",
      "-----------------------------------\n",
      "Training Loss  0.5261001161644968\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5435194023277449\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  692 \n",
      "-----------------------------------\n",
      "Training Loss  0.5260843092136169\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543537261693374\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  693 \n",
      "-----------------------------------\n",
      "Training Loss  0.5260692911871364\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434529094592385\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  694 \n",
      "-----------------------------------\n",
      "Training Loss  0.5260789645521828\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434752391732257\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  695 \n",
      "-----------------------------------\n",
      "Training Loss  0.5260621624046498\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434186458587646\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  696 \n",
      "-----------------------------------\n",
      "Training Loss  0.5260563315300459\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.54342390143353\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  697 \n",
      "-----------------------------------\n",
      "Training Loss  0.5260452199518011\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433616262415181\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  698 \n",
      "-----------------------------------\n",
      "Training Loss  0.5260383463307713\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5432941382345946\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  699 \n",
      "-----------------------------------\n",
      "Training Loss  0.5260032351767079\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5431861229564833\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  700 \n",
      "-----------------------------------\n",
      "Training Loss  0.5259954373488266\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5431294000667074\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  701 \n",
      "-----------------------------------\n",
      "Training Loss  0.5259861021899106\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5431127483430116\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  702 \n",
      "-----------------------------------\n",
      "Training Loss  0.5259547541650493\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5429905329061591\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  703 \n",
      "-----------------------------------\n",
      "Training Loss  0.525919658079576\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5428659436495408\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  704 \n",
      "-----------------------------------\n",
      "Training Loss  0.5258876557430524\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5427716503972593\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  705 \n",
      "-----------------------------------\n",
      "Training Loss  0.5258881047870336\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5427152022071506\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  706 \n",
      "-----------------------------------\n",
      "Training Loss  0.5258654114905368\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5426928880422012\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  707 \n",
      "-----------------------------------\n",
      "Training Loss  0.5258495275224193\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5426467747791953\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  708 \n",
      "-----------------------------------\n",
      "Training Loss  0.525834422767832\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5425257164499034\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  709 \n",
      "-----------------------------------\n",
      "Training Loss  0.5258252968948879\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5425379716831705\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  710 \n",
      "-----------------------------------\n",
      "Training Loss  0.5257803630293085\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5424440373545107\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  711 \n",
      "-----------------------------------\n",
      "Training Loss  0.5257586793283399\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5424821286097817\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  712 \n",
      "-----------------------------------\n",
      "Training Loss  0.5257227966624699\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5424774431664011\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  713 \n",
      "-----------------------------------\n",
      "Training Loss  0.5257002982530701\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5424733278544053\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  714 \n",
      "-----------------------------------\n",
      "Training Loss  0.5256853944130158\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5424581636553225\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  715 \n",
      "-----------------------------------\n",
      "Training Loss  0.5256574217523082\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5424450195353964\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  716 \n",
      "-----------------------------------\n",
      "Training Loss  0.5256553111451395\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5424335054729296\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  717 \n",
      "-----------------------------------\n",
      "Training Loss  0.5256375191586741\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5423775898373645\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  718 \n",
      "-----------------------------------\n",
      "Training Loss  0.5256141501196315\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.542364653037942\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  719 \n",
      "-----------------------------------\n",
      "Training Loss  0.5256086631437366\n",
      "Training Accuracy  0.5990081473609635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss  0.542368137318155\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  720 \n",
      "-----------------------------------\n",
      "Training Loss  0.5255827481827039\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.542317104080449\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  721 \n",
      "-----------------------------------\n",
      "Training Loss  0.525593772363127\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5423231669094252\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  722 \n",
      "-----------------------------------\n",
      "Training Loss  0.5255466244193945\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5422924769961316\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  723 \n",
      "-----------------------------------\n",
      "Training Loss  0.5255502171061012\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.542287537585134\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  724 \n",
      "-----------------------------------\n",
      "Training Loss  0.5255198060126787\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5422853257345117\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  725 \n",
      "-----------------------------------\n",
      "Training Loss  0.5255128275812342\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5423086803892384\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  726 \n",
      "-----------------------------------\n",
      "Training Loss  0.525482479441032\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5423068378282629\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  727 \n",
      "-----------------------------------\n",
      "Training Loss  0.5255051124631689\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5423712769280309\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  728 \n",
      "-----------------------------------\n",
      "Training Loss  0.5254804890477256\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5423752730307372\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  729 \n",
      "-----------------------------------\n",
      "Training Loss  0.5254655852076713\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5424384461796802\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  730 \n",
      "-----------------------------------\n",
      "Training Loss  0.5254486501216888\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5424164559530176\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  731 \n",
      "-----------------------------------\n",
      "Training Loss  0.5254567704173956\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5424531141053075\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  732 \n",
      "-----------------------------------\n",
      "Training Loss  0.5254238619563285\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5424796031868976\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  733 \n",
      "-----------------------------------\n",
      "Training Loss  0.5254201436980387\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5424614095169565\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  734 \n",
      "-----------------------------------\n",
      "Training Loss  0.5254176105006357\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5424601267213407\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  735 \n",
      "-----------------------------------\n",
      "Training Loss  0.5254074405418353\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5424743890762329\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  736 \n",
      "-----------------------------------\n",
      "Training Loss  0.5253942277324334\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5424923313700635\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  737 \n",
      "-----------------------------------\n",
      "Training Loss  0.5254081377152646\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5425812627958215\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  738 \n",
      "-----------------------------------\n",
      "Training Loss  0.5253727573357271\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5425676327684651\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  739 \n",
      "-----------------------------------\n",
      "Training Loss  0.5253777333189932\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5425548553466797\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  740 \n",
      "-----------------------------------\n",
      "Training Loss  0.5253565304734734\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5425982280917789\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  741 \n",
      "-----------------------------------\n",
      "Training Loss  0.5253587174281645\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5425526836644048\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  742 \n",
      "-----------------------------------\n",
      "Training Loss  0.5253492037901718\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5425521822079368\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  743 \n",
      "-----------------------------------\n",
      "Training Loss  0.5253477444809475\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5425665352655493\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  744 \n",
      "-----------------------------------\n",
      "Training Loss  0.5253124665678217\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.542552570933881\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  745 \n",
      "-----------------------------------\n",
      "Training Loss  0.5253281462728308\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5425711546255194\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  746 \n",
      "-----------------------------------\n",
      "Training Loss  0.5253233909606934\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5425822203573973\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  747 \n",
      "-----------------------------------\n",
      "Training Loss  0.5253064384621181\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5426061879033628\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  748 \n",
      "-----------------------------------\n",
      "Training Loss  0.5253153058250298\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5426008118235547\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  749 \n",
      "-----------------------------------\n",
      "Training Loss  0.5253004237507166\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5425873632016389\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  750 \n",
      "-----------------------------------\n",
      "Training Loss  0.5252971779764368\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5425823590029841\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  751 \n",
      "-----------------------------------\n",
      "Training Loss  0.5252942318996686\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.542615700027217\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  752 \n",
      "-----------------------------------\n",
      "Training Loss  0.5252724346819888\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.542677719955859\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  753 \n",
      "-----------------------------------\n",
      "Training Loss  0.5252899735161428\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5426950856395389\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  754 \n",
      "-----------------------------------\n",
      "Training Loss  0.5252795219421387\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5427256278369738\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  755 \n",
      "-----------------------------------\n",
      "Training Loss  0.5252680145622639\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5427410641442174\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  756 \n",
      "-----------------------------------\n",
      "Training Loss  0.5252614289187314\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5428239249664805\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  757 \n",
      "-----------------------------------\n",
      "Training Loss  0.5252444543195575\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5429143283678137\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  758 \n",
      "-----------------------------------\n",
      "Training Loss  0.525204327334179\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5429343944010527\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  759 \n",
      "-----------------------------------\n",
      "Training Loss  0.5252175096715435\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5429309334443964\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  760 \n",
      "-----------------------------------\n",
      "Training Loss  0.5252108704508021\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5429074543973674\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  761 \n",
      "-----------------------------------\n",
      "Training Loss  0.5251793626988872\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5428946497647659\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  762 \n",
      "-----------------------------------\n",
      "Training Loss  0.5251904357685132\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5428710890852887\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  763 \n",
      "-----------------------------------\n",
      "Training Loss  0.5251980778876315\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.542906800042028\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  764 \n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss  0.5251610667518015\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5428125573241193\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  765 \n",
      "-----------------------------------\n",
      "Training Loss  0.5251766510224074\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5427631722844165\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  766 \n",
      "-----------------------------------\n",
      "Training Loss  0.5251602048284552\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5427540035351462\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  767 \n",
      "-----------------------------------\n",
      "Training Loss  0.5251630846034275\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5427358046821926\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  768 \n",
      "-----------------------------------\n",
      "Training Loss  0.5251457028174669\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5427401402722234\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  769 \n",
      "-----------------------------------\n",
      "Training Loss  0.5251382793603319\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5427499646725862\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  770 \n",
      "-----------------------------------\n",
      "Training Loss  0.5251378651415364\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5427591178728186\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  771 \n",
      "-----------------------------------\n",
      "Training Loss  0.5251273700360501\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5427825230619182\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  772 \n",
      "-----------------------------------\n",
      "Training Loss  0.5251346116655329\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5428180616834889\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  773 \n",
      "-----------------------------------\n",
      "Training Loss  0.5250994713788621\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5428346550982931\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  774 \n",
      "-----------------------------------\n",
      "Training Loss  0.5251116608635763\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5428577557854031\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  775 \n",
      "-----------------------------------\n",
      "Training Loss  0.525110958332426\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5428802526515463\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  776 \n",
      "-----------------------------------\n",
      "Training Loss  0.525089400537898\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.542858547490576\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  777 \n",
      "-----------------------------------\n",
      "Training Loss  0.5250825399763128\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5428798341232798\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  778 \n",
      "-----------------------------------\n",
      "Training Loss  0.5250690053688006\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.542914083470469\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  779 \n",
      "-----------------------------------\n",
      "Training Loss  0.5250606433059393\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5429271615069845\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  780 \n",
      "-----------------------------------\n",
      "Training Loss  0.5250609296091487\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5429279234098352\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  781 \n",
      "-----------------------------------\n",
      "Training Loss  0.525053458602241\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5429266833740732\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  782 \n",
      "-----------------------------------\n",
      "Training Loss  0.5250590768422974\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5429439609465392\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  783 \n",
      "-----------------------------------\n",
      "Training Loss  0.5250422288192792\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5429919362068176\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  784 \n",
      "-----------------------------------\n",
      "Training Loss  0.5250452351704072\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.542975629153459\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  785 \n",
      "-----------------------------------\n",
      "Training Loss  0.525029688738705\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5430202872856803\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  786 \n",
      "-----------------------------------\n",
      "Training Loss  0.5250235424282845\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543049869330033\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  787 \n",
      "-----------------------------------\n",
      "Training Loss  0.5250300671277421\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5430526513120403\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  788 \n",
      "-----------------------------------\n",
      "Training Loss  0.5250206653321727\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5430736425130264\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  789 \n",
      "-----------------------------------\n",
      "Training Loss  0.5250188216064753\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5430814934813458\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  790 \n",
      "-----------------------------------\n",
      "Training Loss  0.5250063876757461\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5431013638558595\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  791 \n",
      "-----------------------------------\n",
      "Training Loss  0.5250121361753913\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5431056152219358\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  792 \n",
      "-----------------------------------\n",
      "Training Loss  0.5250037114941672\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5431220052034959\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  793 \n",
      "-----------------------------------\n",
      "Training Loss  0.524994970037696\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5431114953497181\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  794 \n",
      "-----------------------------------\n",
      "Training Loss  0.5249913184160597\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5431585117526676\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  795 \n",
      "-----------------------------------\n",
      "Training Loss  0.5249866866663601\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5431701437286709\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  796 \n",
      "-----------------------------------\n",
      "Training Loss  0.5249823978107967\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543197064296059\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  797 \n",
      "-----------------------------------\n",
      "Training Loss  0.52497566482994\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5432063431843467\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  798 \n",
      "-----------------------------------\n",
      "Training Loss  0.524976451745194\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5432286236597144\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  799 \n",
      "-----------------------------------\n",
      "Training Loss  0.5249584846282274\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5431985453419064\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  800 \n",
      "-----------------------------------\n",
      "Training Loss  0.5249610097890489\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5432141060414521\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  801 \n",
      "-----------------------------------\n",
      "Training Loss  0.5249632077940395\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5432288802188375\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  802 \n",
      "-----------------------------------\n",
      "Training Loss  0.5249449958962001\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5432637683723284\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  803 \n",
      "-----------------------------------\n",
      "Training Loss  0.5249570380435901\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5432587123435476\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  804 \n",
      "-----------------------------------\n",
      "Training Loss  0.524951973323072\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5432525704736295\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  805 \n",
      "-----------------------------------\n",
      "Training Loss  0.5249388646543696\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5432293104088824\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  806 \n",
      "-----------------------------------\n",
      "Training Loss  0.5249391723884625\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5432446145493052\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  807 \n",
      "-----------------------------------\n",
      "Training Loss  0.5249310825647933\n",
      "Training Accuracy  0.5990081473609635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss  0.5432329579539921\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  808 \n",
      "-----------------------------------\n",
      "Training Loss  0.5249436209710796\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5432764434296152\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  809 \n",
      "-----------------------------------\n",
      "Training Loss  0.5249210642964652\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543290402578271\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  810 \n",
      "-----------------------------------\n",
      "Training Loss  0.524919812598925\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5432682387206865\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  811 \n",
      "-----------------------------------\n",
      "Training Loss  0.5249168819256043\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5432629844416743\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  812 \n",
      "-----------------------------------\n",
      "Training Loss  0.5249248625857107\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5432922023793926\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  813 \n",
      "-----------------------------------\n",
      "Training Loss  0.5249231913116541\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433074987452963\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  814 \n",
      "-----------------------------------\n",
      "Training Loss  0.5248929124869658\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5432897184206091\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  815 \n",
      "-----------------------------------\n",
      "Training Loss  0.5249090157867817\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433176963225655\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  816 \n",
      "-----------------------------------\n",
      "Training Loss  0.5249121564157894\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433229894741721\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  817 \n",
      "-----------------------------------\n",
      "Training Loss  0.524894651737106\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5432898000530575\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  818 \n",
      "-----------------------------------\n",
      "Training Loss  0.5249009698294522\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433063727358113\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  819 \n",
      "-----------------------------------\n",
      "Training Loss  0.5248935128865617\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433232641738394\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  820 \n",
      "-----------------------------------\n",
      "Training Loss  0.5248880091677891\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433243927748307\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  821 \n",
      "-----------------------------------\n",
      "Training Loss  0.5248954654409644\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433235181414563\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  822 \n",
      "-----------------------------------\n",
      "Training Loss  0.5248958458391468\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433376586955526\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  823 \n",
      "-----------------------------------\n",
      "Training Loss  0.5248830968074585\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5432924537555032\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  824 \n",
      "-----------------------------------\n",
      "Training Loss  0.5248913574084807\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433020786098812\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  825 \n",
      "-----------------------------------\n",
      "Training Loss  0.5248713098215253\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433098570160244\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  826 \n",
      "-----------------------------------\n",
      "Training Loss  0.5248828125133943\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433147860609967\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  827 \n",
      "-----------------------------------\n",
      "Training Loss  0.5248857063523839\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433847735757413\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  828 \n",
      "-----------------------------------\n",
      "Training Loss  0.5248768108614376\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434043731378473\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  829 \n",
      "-----------------------------------\n",
      "Training Loss  0.5248710215091705\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433851985827737\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  830 \n",
      "-----------------------------------\n",
      "Training Loss  0.5248731140340313\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434055237666421\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  831 \n",
      "-----------------------------------\n",
      "Training Loss  0.524867075882601\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434060589126919\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  832 \n",
      "-----------------------------------\n",
      "Training Loss  0.5248588196347269\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433992484341497\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  833 \n",
      "-----------------------------------\n",
      "Training Loss  0.524873673915863\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543409640374391\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  834 \n",
      "-----------------------------------\n",
      "Training Loss  0.52486196361231\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434107262155284\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  835 \n",
      "-----------------------------------\n",
      "Training Loss  0.5248499623175418\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433700965798419\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  836 \n",
      "-----------------------------------\n",
      "Training Loss  0.524860082047709\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433803615362748\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  837 \n",
      "-----------------------------------\n",
      "Training Loss  0.5248546382684386\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433799520782803\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  838 \n",
      "-----------------------------------\n",
      "Training Loss  0.5248449391193604\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433649900166885\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  839 \n",
      "-----------------------------------\n",
      "Training Loss  0.5248443755540955\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433279910813207\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  840 \n",
      "-----------------------------------\n",
      "Training Loss  0.5248343676663516\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433101718840392\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  841 \n",
      "-----------------------------------\n",
      "Training Loss  0.5248355969284357\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543302270381347\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  842 \n",
      "-----------------------------------\n",
      "Training Loss  0.5248424541414454\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5432976562043895\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  843 \n",
      "-----------------------------------\n",
      "Training Loss  0.5248274133446511\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5432988262694814\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  844 \n",
      "-----------------------------------\n",
      "Training Loss  0.5248368891437402\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433191864386849\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  845 \n",
      "-----------------------------------\n",
      "Training Loss  0.5248228278052941\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5432764823022096\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  846 \n",
      "-----------------------------------\n",
      "Training Loss  0.5248230920079049\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5432787330254264\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  847 \n",
      "-----------------------------------\n",
      "Training Loss  0.5248317581214262\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543319494827934\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  848 \n",
      "-----------------------------------\n",
      "Training Loss  0.5248112343670277\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433065826478212\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  849 \n",
      "-----------------------------------\n",
      "Training Loss  0.5248167531543904\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5432992370232291\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  850 \n",
      "-----------------------------------\n",
      "Training Loss  0.5248221560810389\n",
      "Training Accuracy  0.5990081473609635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss  0.5433353470719379\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  851 \n",
      "-----------------------------------\n",
      "Training Loss  0.5248144573709937\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433357668959576\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  852 \n",
      "-----------------------------------\n",
      "Training Loss  0.524783109011275\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433375291202379\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  853 \n",
      "-----------------------------------\n",
      "Training Loss  0.5248232480515255\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433646168397821\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  854 \n",
      "-----------------------------------\n",
      "Training Loss  0.5248076587580564\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433690586815709\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  855 \n",
      "-----------------------------------\n",
      "Training Loss  0.5247875776853455\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433479832566303\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  856 \n",
      "-----------------------------------\n",
      "Training Loss  0.5247960455631941\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433651830839075\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  857 \n",
      "-----------------------------------\n",
      "Training Loss  0.5248021459981297\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433855704639269\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  858 \n",
      "-----------------------------------\n",
      "Training Loss  0.5247959528076515\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433663363042085\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  859 \n",
      "-----------------------------------\n",
      "Training Loss  0.5247965347900819\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543344936940981\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  860 \n",
      "-----------------------------------\n",
      "Training Loss  0.5247855062565107\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433421186778856\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  861 \n",
      "-----------------------------------\n",
      "Training Loss  0.524805989493145\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433561387269393\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  862 \n",
      "-----------------------------------\n",
      "Training Loss  0.5247710347175598\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433355867862701\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  863 \n",
      "-----------------------------------\n",
      "Training Loss  0.524791407451201\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433597124141195\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  864 \n",
      "-----------------------------------\n",
      "Training Loss  0.5247939868589465\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433582611705946\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  865 \n",
      "-----------------------------------\n",
      "Training Loss  0.5247597269127878\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433346020138782\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  866 \n",
      "-----------------------------------\n",
      "Training Loss  0.524776102451796\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433501342068547\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  867 \n",
      "-----------------------------------\n",
      "Training Loss  0.5247931828659572\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433615601581075\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  868 \n",
      "-----------------------------------\n",
      "Training Loss  0.5247605834784133\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543351913275926\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  869 \n",
      "-----------------------------------\n",
      "Training Loss  0.5247766964890984\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433688500653142\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  870 \n",
      "-----------------------------------\n",
      "Training Loss  0.5247689231058185\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433832108974457\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  871 \n",
      "-----------------------------------\n",
      "Training Loss  0.5247620843099744\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433899552925773\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  872 \n",
      "-----------------------------------\n",
      "Training Loss  0.5247726370109601\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543405206307121\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  873 \n",
      "-----------------------------------\n",
      "Training Loss  0.5247641751605473\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433945331884467\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  874 \n",
      "-----------------------------------\n",
      "Training Loss  0.5247621964872553\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433800648088041\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  875 \n",
      "-----------------------------------\n",
      "Training Loss  0.524769337994329\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433762656605762\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  876 \n",
      "-----------------------------------\n",
      "Training Loss  0.5247439742088318\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433867806973665\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  877 \n",
      "-----------------------------------\n",
      "Training Loss  0.5247783138510886\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543404107508452\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  878 \n",
      "-----------------------------------\n",
      "Training Loss  0.5247487057460828\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433981548184934\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  879 \n",
      "-----------------------------------\n",
      "Training Loss  0.5247558285011334\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543398362138997\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  880 \n",
      "-----------------------------------\n",
      "Training Loss  0.5247390444359082\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434085338012032\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  881 \n",
      "-----------------------------------\n",
      "Training Loss  0.5247568903344401\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433830165344736\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  882 \n",
      "-----------------------------------\n",
      "Training Loss  0.5247532939643003\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433903129204459\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  883 \n",
      "-----------------------------------\n",
      "Training Loss  0.5247406340047215\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433935225009918\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  884 \n",
      "-----------------------------------\n",
      "Training Loss  0.5247439718648289\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543396708757981\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  885 \n",
      "-----------------------------------\n",
      "Training Loss  0.524735042888127\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543358088835426\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  886 \n",
      "-----------------------------------\n",
      "Training Loss  0.524745563777645\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433818555396536\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  887 \n",
      "-----------------------------------\n",
      "Training Loss  0.5247362507193276\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433738827705383\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  888 \n",
      "-----------------------------------\n",
      "Training Loss  0.5247359215543511\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434171259403229\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  889 \n",
      "-----------------------------------\n",
      "Training Loss  0.5247370878632149\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543386486561402\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  890 \n",
      "-----------------------------------\n",
      "Training Loss  0.5247436216038265\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433843382026838\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  891 \n",
      "-----------------------------------\n",
      "Training Loss  0.5247195962439762\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433585216169772\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  892 \n",
      "-----------------------------------\n",
      "Training Loss  0.5247167847799451\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433801710605621\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  893 \n",
      "-----------------------------------\n",
      "Training Loss  0.5247523543540011\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433990320433741\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  894 \n",
      "-----------------------------------\n",
      "Training Loss  0.5247158317753439\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434090404406838\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  895 \n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss  0.5247401400898279\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543417549651602\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  896 \n",
      "-----------------------------------\n",
      "Training Loss  0.5247162610627292\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5433875270511793\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  897 \n",
      "-----------------------------------\n",
      "Training Loss  0.5247220306584005\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434169108453004\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  898 \n",
      "-----------------------------------\n",
      "Training Loss  0.5247337027881922\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543404722991197\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  899 \n",
      "-----------------------------------\n",
      "Training Loss  0.5247060428844409\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434016067048778\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  900 \n",
      "-----------------------------------\n",
      "Training Loss  0.5247121590576814\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434582116811172\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  901 \n",
      "-----------------------------------\n",
      "Training Loss  0.5247263784488935\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434279351130776\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  902 \n",
      "-----------------------------------\n",
      "Training Loss  0.5247202860505393\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434505330479663\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  903 \n",
      "-----------------------------------\n",
      "Training Loss  0.524703644299775\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434294174546781\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  904 \n",
      "-----------------------------------\n",
      "Training Loss  0.5247337965483077\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434845297232919\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  905 \n",
      "-----------------------------------\n",
      "Training Loss  0.5247044007429916\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543433032605959\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  906 \n",
      "-----------------------------------\n",
      "Training Loss  0.5247099124983455\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434493927851968\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  907 \n",
      "-----------------------------------\n",
      "Training Loss  0.524713969632481\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434731957705125\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  908 \n",
      "-----------------------------------\n",
      "Training Loss  0.5247087461894817\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434631977392279\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  909 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246974199675443\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5435005841047867\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  910 \n",
      "-----------------------------------\n",
      "Training Loss  0.5247268941295281\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434912248798038\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  911 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246866964892055\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434919543888258\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  912 \n",
      "-----------------------------------\n",
      "Training Loss  0.5247064815478378\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5435009935627813\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  913 \n",
      "-----------------------------------\n",
      "Training Loss  0.5247067651721868\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5435035980266073\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  914 \n",
      "-----------------------------------\n",
      "Training Loss  0.5247010370988524\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5434985057167385\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  915 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246977548250992\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543536171964977\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  916 \n",
      "-----------------------------------\n",
      "Training Loss  0.524692607394765\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5435103061406509\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  917 \n",
      "-----------------------------------\n",
      "Training Loss  0.5247008773717987\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5435456452162369\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  918 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246942857008302\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5435193777084351\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  919 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246967371929897\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543521690627803\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  920 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246867460481236\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5435432480729144\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  921 \n",
      "-----------------------------------\n",
      "Training Loss  0.5247038341640087\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5435622075329656\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  922 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246887347671423\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5435223734897116\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  923 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246768731079744\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5435553413370381\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  924 \n",
      "-----------------------------------\n",
      "Training Loss  0.5247001490566168\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5435433011987935\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  925 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246774473886812\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5435807691968005\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  926 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246935687708051\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5435843066028927\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  927 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246880027685272\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5435933535513671\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  928 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246832745798519\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5435819574024366\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  929 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246838341268261\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5436119825943656\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  930 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246983341286692\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5436409932115803\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  931 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246705593687765\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5435991740745046\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  932 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246759174245127\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5436272465664408\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  933 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246853081697829\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5436181231685306\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  934 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246677586201871\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5436311999092931\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  935 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246809131643745\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5436381723569788\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  936 \n",
      "-----------------------------------\n",
      "Training Loss  0.524670864424009\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5436419689137003\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  937 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246805856736858\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5436371798100679\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  938 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246587985017327\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543656218311061\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  939 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246758112746678\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.54368842555129\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  940 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246844137652537\n",
      "Training Accuracy  0.5990081473609635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss  0.5436495024224987\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  941 \n",
      "-----------------------------------\n",
      "Training Loss  0.524669657597381\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5436617472897405\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  942 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246635494607218\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5436598801094553\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  943 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246640373481793\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.54367486808611\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  944 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246659547425387\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5437021696049235\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  945 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246811626332529\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5436915923719821\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  946 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246540642856212\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543692198784455\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  947 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246614602844367\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543671366961106\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  948 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246637349718073\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5437201404053232\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  949 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246492520476995\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5437400146670963\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  950 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246694841411677\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5437387318714805\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  951 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246700376607059\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5437508821487427\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  952 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246582523490606\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5437230584414109\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  953 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246545518382212\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5437431659387506\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  954 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246566684728258\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5437451251175093\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  955 \n",
      "-----------------------------------\n",
      "Training Loss  0.524655880552999\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5437409838904506\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  956 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246491686681684\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5437308614668639\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  957 \n",
      "-----------------------------------\n",
      "Training Loss  0.524665959095687\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5437953925651052\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  958 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246656647558963\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5437358617782593\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  959 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246327288365096\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5437571380449377\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  960 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246568764193674\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5437557839828989\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  961 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246566678031107\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5437906306722889\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  962 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246466200673179\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5437823857950128\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  963 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246464241756482\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5437605666077655\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  964 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246445881516746\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5437871956306956\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  965 \n",
      "-----------------------------------\n",
      "Training Loss  0.524642053949699\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5438203993050948\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  966 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246481831823842\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5437865788521974\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  967 \n",
      "-----------------------------------\n",
      "Training Loss  0.524631703837534\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5437865024027617\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  968 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246500503481104\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5438319794509722\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  969 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246477016572202\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5438229350940041\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  970 \n",
      "-----------------------------------\n",
      "Training Loss  0.524643194474531\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5437724331150884\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  971 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246284192197779\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5437965108000714\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  972 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246350537525134\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5437674315079398\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  973 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246419136443835\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5437697949616805\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  974 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246313063616164\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5437711593897446\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  975 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246211742417196\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5437676116176273\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  976 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246319810995895\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543725708256597\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  977 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246349402358023\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5437681454679241\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  978 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246256834335541\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5437484526115915\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  979 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246253391999877\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5437498507292374\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  980 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246276326393813\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5437325355799302\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  981 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246259292189994\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5436936863090681\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  982 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246110441309683\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5437220399794371\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  983 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246206351210562\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5436936539152394\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  984 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246177804604005\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5437376214110333\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  985 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246334926465924\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5436943471431732\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  986 \n",
      "-----------------------------------\n",
      "Training Loss  0.5245943210098181\n",
      "Training Accuracy  0.5990081473609635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss  0.5436856487522954\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  987 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246269558922628\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5437292327051577\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  988 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246125051144803\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5436980607716934\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  989 \n",
      "-----------------------------------\n",
      "Training Loss  0.524609085214272\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5436694634997327\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  990 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246263491303733\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5437144442744877\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  991 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246009779780099\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5436952749024266\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  992 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246158952123663\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5437310104784758\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  993 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246014886357812\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5436724644640217\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  994 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246131748295901\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5436929580957993\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  995 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246153189225143\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5437295994032985\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  996 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246093326740051\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5437042285566744\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  997 \n",
      "-----------------------------------\n",
      "Training Loss  0.5245956751737701\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5437312735163647\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  998 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246143438173144\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5436954394630764\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  999 \n",
      "-----------------------------------\n",
      "Training Loss  0.5246042013168335\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.5437315210052158\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Epoch  1000 \n",
      "-----------------------------------\n",
      "Training Loss  0.5245935726031828\n",
      "Training Accuracy  0.5990081473609635\n",
      "Testing loss  0.543719110281571\n",
      "Testing Accuracy  0.5821529745042493\n",
      "\n",
      "Testing loss  0.552006296813488\n",
      "Testing Accuracy  0.5956964892412231\n"
     ]
    }
   ],
   "source": [
    "Model = PNeuralNetwork()\n",
    "Loss_fn = nn.BCELoss() #nn.BCELoss\n",
    "Optimizer = torch.optim.Adam(Model.parameters())\n",
    "\n",
    "TrainingAcc = []\n",
    "TrainingLoss = []\n",
    "ValidationAcc = []\n",
    "ValidationLoss = []\n",
    "\n",
    "for e in range(1000):\n",
    "    print(\"Epoch \", e+1, \"\\n-----------------------------------\")\n",
    "    TLoss, TAcc = pTraining(Train_dataloader, Model, Loss_fn, Optimizer)\n",
    "    TrainingAcc.append(TAcc)\n",
    "    TrainingLoss.append(TLoss)\n",
    "    VLoss, VAcc, x = pTesting(Val_dataloader, Model, Loss_fn)\n",
    "    ValidationAcc.append(VAcc)\n",
    "    ValidationLoss.append(VLoss)\n",
    "    print()\n",
    "x, y, PredLabel = pTesting(Test_dataloader, Model, Loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "a2c2d875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x134a2c7ae50>"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(TrainingAcc)\n",
    "plt.plot(ValidationAcc)\n",
    "plt.grid()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train','Validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "71690f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x134a2c7af10>"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(TrainingLoss)\n",
    "plt.plot(ValidationLoss)\n",
    "plt.grid()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train','Validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "718a7a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       357\n",
      "           1       0.60      1.00      0.75       526\n",
      "\n",
      "    accuracy                           0.60       883\n",
      "   macro avg       0.30      0.50      0.37       883\n",
      "weighted avg       0.35      0.60      0.44       883\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ouedr\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ouedr\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ouedr\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(TestLabel, PredLabel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "36fbc802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(33.0, 0.5, 'Truth')"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Conf_mat = confusion_matrix(TestLabel, PredLabel)\n",
    "\n",
    "sn.heatmap(Conf_mat, annot=True, cmap='BuPu', fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f2a2deb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No early stopping will be performed, last training weights will be used.\n"
     ]
    }
   ],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "\n",
    "classifier = TabNetClassifier(verbose=0,seed=42)\n",
    "classifier.fit(X_train=Train, y_train=TrainLabel,\n",
    "               patience=5,max_epochs=100,\n",
    "               eval_metric=['auc'])\n",
    "\n",
    "predictions = classifier.predict_proba(Test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f35b23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
